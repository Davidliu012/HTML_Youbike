{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7106821,"sourceType":"datasetVersion","datasetId":4097269},{"sourceId":7107818,"sourceType":"datasetVersion","datasetId":4097960},{"sourceId":7111750,"sourceType":"datasetVersion","datasetId":4100724},{"sourceId":7112592,"sourceType":"datasetVersion","datasetId":4101363}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T05:26:57.893954Z","iopub.execute_input":"2023-12-04T05:26:57.894247Z","iopub.status.idle":"2023-12-04T05:26:58.284221Z","shell.execute_reply.started":"2023-12-04T05:26:57.894220Z","shell.execute_reply":"2023-12-04T05:26:58.283315Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset-filled/dataset_filled/500101216.csv\n/kaggle/input/dataset-filled/dataset_filled/500101114.csv\n/kaggle/input/dataset-filled/dataset_filled/500119046.csv\n/kaggle/input/dataset-filled/dataset_filled/500119074.csv\n/kaggle/input/dataset-filled/dataset_filled/500101092.csv\n/kaggle/input/dataset-filled/dataset_filled/500119086.csv\n/kaggle/input/dataset-filled/dataset_filled/500101042.csv\n/kaggle/input/dataset-filled/dataset_filled/500119043.csv\n/kaggle/input/dataset-filled/dataset_filled/500101093.csv\n/kaggle/input/dataset-filled/dataset_filled/500101010.csv\n/kaggle/input/dataset-filled/dataset_filled/500101008.csv\n/kaggle/input/dataset-filled/dataset_filled/500101015.csv\n/kaggle/input/dataset-filled/dataset_filled/500101040.csv\n/kaggle/input/dataset-filled/dataset_filled/500101094.csv\n/kaggle/input/dataset-filled/dataset_filled/500101037.csv\n/kaggle/input/dataset-filled/dataset_filled/500119048.csv\n/kaggle/input/dataset-filled/dataset_filled/500119089.csv\n/kaggle/input/dataset-filled/dataset_filled/500119087.csv\n/kaggle/input/dataset-filled/dataset_filled/500101023.csv\n/kaggle/input/dataset-filled/dataset_filled/500101188.csv\n/kaggle/input/dataset-filled/dataset_filled/500119071.csv\n/kaggle/input/dataset-filled/dataset_filled/500101020.csv\n/kaggle/input/dataset-filled/dataset_filled/500119059.csv\n/kaggle/input/dataset-filled/dataset_filled/500101013.csv\n/kaggle/input/dataset-filled/dataset_filled/500101115.csv\n/kaggle/input/dataset-filled/dataset_filled/500119088.csv\n/kaggle/input/dataset-filled/dataset_filled/500101034.csv\n/kaggle/input/dataset-filled/dataset_filled/500101091.csv\n/kaggle/input/dataset-filled/dataset_filled/500101191.csv\n/kaggle/input/dataset-filled/dataset_filled/500119085.csv\n/kaggle/input/dataset-filled/dataset_filled/500101189.csv\n/kaggle/input/dataset-filled/dataset_filled/500119066.csv\n/kaggle/input/dataset-filled/dataset_filled/500106003.csv\n/kaggle/input/dataset-filled/dataset_filled/500101002.csv\n/kaggle/input/dataset-filled/dataset_filled/500119077.csv\n/kaggle/input/dataset-filled/dataset_filled/500101041.csv\n/kaggle/input/dataset-filled/dataset_filled/500101024.csv\n/kaggle/input/dataset-filled/dataset_filled/500101181.csv\n/kaggle/input/dataset-filled/dataset_filled/500119053.csv\n/kaggle/input/dataset-filled/dataset_filled/500119052.csv\n/kaggle/input/dataset-filled/dataset_filled/500101199.csv\n/kaggle/input/dataset-filled/dataset_filled/500106002.csv\n/kaggle/input/dataset-filled/dataset_filled/500119080.csv\n/kaggle/input/dataset-filled/dataset_filled/500101209.csv\n/kaggle/input/dataset-filled/dataset_filled/500101036.csv\n/kaggle/input/dataset-filled/dataset_filled/500101038.csv\n/kaggle/input/dataset-filled/dataset_filled/500101030.csv\n/kaggle/input/dataset-filled/dataset_filled/500119054.csv\n/kaggle/input/dataset-filled/dataset_filled/500101014.csv\n/kaggle/input/dataset-filled/dataset_filled/500119056.csv\n/kaggle/input/dataset-filled/dataset_filled/500119069.csv\n/kaggle/input/dataset-filled/dataset_filled/500119060.csv\n/kaggle/input/dataset-filled/dataset_filled/500101123.csv\n/kaggle/input/dataset-filled/dataset_filled/500101001.csv\n/kaggle/input/dataset-filled/dataset_filled/500119084.csv\n/kaggle/input/dataset-filled/dataset_filled/500101032.csv\n/kaggle/input/dataset-filled/dataset_filled/500101003.csv\n/kaggle/input/dataset-filled/dataset_filled/500119083.csv\n/kaggle/input/dataset-filled/dataset_filled/500101039.csv\n/kaggle/input/dataset-filled/dataset_filled/500119055.csv\n/kaggle/input/dataset-filled/dataset_filled/500101007.csv\n/kaggle/input/dataset-filled/dataset_filled/500101027.csv\n/kaggle/input/dataset-filled/dataset_filled/500101176.csv\n/kaggle/input/dataset-filled/dataset_filled/500119058.csv\n/kaggle/input/dataset-filled/dataset_filled/500119091.csv\n/kaggle/input/dataset-filled/dataset_filled/500119065.csv\n/kaggle/input/dataset-filled/dataset_filled/500119050.csv\n/kaggle/input/dataset-filled/dataset_filled/500119064.csv\n/kaggle/input/dataset-filled/dataset_filled/500101005.csv\n/kaggle/input/dataset-filled/dataset_filled/500119061.csv\n/kaggle/input/dataset-filled/dataset_filled/500119082.csv\n/kaggle/input/dataset-filled/dataset_filled/500101019.csv\n/kaggle/input/dataset-filled/dataset_filled/500119063.csv\n/kaggle/input/dataset-filled/dataset_filled/500119067.csv\n/kaggle/input/dataset-filled/dataset_filled/500101184.csv\n/kaggle/input/dataset-filled/dataset_filled/500119076.csv\n/kaggle/input/dataset-filled/dataset_filled/500119078.csv\n/kaggle/input/dataset-filled/dataset_filled/500119057.csv\n/kaggle/input/dataset-filled/dataset_filled/500101035.csv\n/kaggle/input/dataset-filled/dataset_filled/500101166.csv\n/kaggle/input/dataset-filled/dataset_filled/500101004.csv\n/kaggle/input/dataset-filled/dataset_filled/500101193.csv\n/kaggle/input/dataset-filled/dataset_filled/500119045.csv\n/kaggle/input/dataset-filled/dataset_filled/500101018.csv\n/kaggle/input/dataset-filled/dataset_filled/500119062.csv\n/kaggle/input/dataset-filled/dataset_filled/500119079.csv\n/kaggle/input/dataset-filled/dataset_filled/500106004.csv\n/kaggle/input/dataset-filled/dataset_filled/500119049.csv\n/kaggle/input/dataset-filled/dataset_filled/500101031.csv\n/kaggle/input/dataset-filled/dataset_filled/500119081.csv\n/kaggle/input/dataset-filled/dataset_filled/500101009.csv\n/kaggle/input/dataset-filled/dataset_filled/500119044.csv\n/kaggle/input/dataset-filled/dataset_filled/500101022.csv\n/kaggle/input/dataset-filled/dataset_filled/500101185.csv\n/kaggle/input/dataset-filled/dataset_filled/500101026.csv\n/kaggle/input/dataset-filled/dataset_filled/500119070.csv\n/kaggle/input/dataset-filled/dataset_filled/500119075.csv\n/kaggle/input/dataset-filled/dataset_filled/500101190.csv\n/kaggle/input/dataset-filled/dataset_filled/500101033.csv\n/kaggle/input/dataset-filled/dataset_filled/500101028.csv\n/kaggle/input/dataset-filled/dataset_filled/500119090.csv\n/kaggle/input/dataset-filled/dataset_filled/500101021.csv\n/kaggle/input/dataset-filled/dataset_filled/500119072.csv\n/kaggle/input/dataset-filled/dataset_filled/500101025.csv\n/kaggle/input/dataset-filled/dataset_filled/500101029.csv\n/kaggle/input/dataset-filled/dataset_filled/500101219.csv\n/kaggle/input/dataset-filled/dataset_filled/500101006.csv\n/kaggle/input/dataset-filled/dataset_filled/500119047.csv\n/kaggle/input/dataset-filled/dataset_filled/500119068.csv\n/kaggle/input/dataset-filled/dataset_filled/500119051.csv\n/kaggle/input/dataset-filled/dataset_filled/500101175.csv\n/kaggle/input/dataset-filled/dataset_filled/500105066.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:26:58.286041Z","iopub.execute_input":"2023-12-04T05:26:58.286410Z","iopub.status.idle":"2023-12-04T05:27:02.165490Z","shell.execute_reply.started":"2023-12-04T05:26:58.286384Z","shell.execute_reply":"2023-12-04T05:27:02.164443Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_dataset(dataset, lookback):\n    \"\"\"Transform a time series into a prediction dataset\n    \n    Args:\n        dataset: A numpy array of time series, first dimension is the time steps\n        lookback: Size of window for prediction\n    \"\"\"\n    X, y = [], []\n    for i in range(len(dataset)-lookback):\n        feature = dataset[i:i+lookback]\n        target = dataset[i+1:i+lookback+1]\n        X.append(feature)\n        y.append(target)\n    return torch.tensor(X), torch.tensor(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.166800Z","iopub.execute_input":"2023-12-04T05:27:02.167243Z","iopub.status.idle":"2023-12-04T05:27:02.173886Z","shell.execute_reply.started":"2023-12-04T05:27:02.167215Z","shell.execute_reply":"2023-12-04T05:27:02.172955Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def train_valid_split(timeseries):\n    # train-valid split with 10/2~11/16 [0:2734], 11/17~11/30[2734:]\n    # 10/2 ~ 10/11 [:711]\n    train_size = 2734\n    valid_size = len(timeseries) - train_size\n    train, valid = timeseries[:train_size], timeseries[train_size:]\n    print(len(train), len(valid))\n    # window size approximately = 3 days\n    lookback = 72 * 2\n    X_train, y_train = create_dataset(train, lookback=lookback)\n    X_valid, y_valid = create_dataset(valid, lookback=lookback)\n    # X_test, y_test = create_dataset(test, lookback=lookback)\n    print(X_train.shape, y_train.shape)\n    print(X_valid.shape, y_valid.shape)\n    return X_train, y_train, X_valid, y_valid","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.177402Z","iopub.execute_input":"2023-12-04T05:27:02.177699Z","iopub.status.idle":"2023-12-04T05:27:02.192467Z","shell.execute_reply.started":"2023-12-04T05:27:02.177673Z","shell.execute_reply":"2023-12-04T05:27:02.191657Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Build a basic RNN model","metadata":{}},{"cell_type":"code","source":"class UBikeRNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True)    \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        # h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n#         print(x.size(0)) # x.size(0) = batch size\n        hidden = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n        x = x.view(len(x),1,-1)\n#         print(x.shape)\n        # Passing in the input and hidden state into the model and obtaining outputs\n        out, hidden = self.rnn(x, hidden)\n        \n        # Reshaping the outputs such that it can be fit into the fully connected layer\n        out = out.contiguous().view(-1, self.hidden_dim)\n        out = self.fc(out)\n        out = self.sig(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.193841Z","iopub.execute_input":"2023-12-04T05:27:02.194460Z","iopub.status.idle":"2023-12-04T05:27:02.205844Z","shell.execute_reply.started":"2023-12-04T05:27:02.194425Z","shell.execute_reply":"2023-12-04T05:27:02.204944Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class UBikeLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = layer_dim\n        self.output_size = output_dim\n        \n        # initialize LSTM   \n        self.block = nn.Sequential(\n            nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Sigmoid(),\n        )\n        \n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True)\n        # the output of LSTM is classified by linear and sigmoid functions\n        self.linear = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        h_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # hidden state\n        c_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # internal state\n        x = x.view(len(x),1,-1)\n        lstm_out, (hn, cn)  = self.lstm(x, (h_0.detach(), c_0.detach()))\n        hn = hn.view(-1, self.hidden_dim) # reshaping the data for Dense layer next\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.linear(lstm_out)\n        out = self.sigmoid(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.206743Z","iopub.execute_input":"2023-12-04T05:27:02.206994Z","iopub.status.idle":"2023-12-04T05:27:02.217442Z","shell.execute_reply.started":"2023-12-04T05:27:02.206971Z","shell.execute_reply":"2023-12-04T05:27:02.216345Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"code","source":"def getLoss(pred, label):\n    loss = torch.mean(3 * torch.abs(pred-label) * (torch.abs(pred-1/3) + torch.abs(pred-2/3)))\n#     loss = torch.mean(torch.square(pred-label))\n#     print(f\"loss = {loss}\")\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.218613Z","iopub.execute_input":"2023-12-04T05:27:02.218898Z","iopub.status.idle":"2023-12-04T05:27:02.228337Z","shell.execute_reply.started":"2023-12-04T05:27:02.218873Z","shell.execute_reply":"2023-12-04T05:27:02.227573Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 32,\n    \"data_filepath\": '/kaggle/input/stop001-sorted/500101001_sorted.csv',\n    \"inference_filepath\": '/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv',\n    \"epochs\": 500,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 5e-3,\n    \"save_dir\": \"/kaggle/working/\",\n    \"model_name\": \"stop001-RNN-v1.ckpt\",\n    \"early_stop\": 150,\n}\n# model parameters\nwindow_size = 72 * 2\ninput_dim = window_size\nhidden_dim = 1024   # the hidden dim\nlayer_dim = 3   # the number of hidden layers\noutput_dim = window_size\ndrop_prob = 0.2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.229529Z","iopub.execute_input":"2023-12-04T05:27:02.229812Z","iopub.status.idle":"2023-12-04T05:27:02.259012Z","shell.execute_reply.started":"2023-12-04T05:27:02.229788Z","shell.execute_reply":"2023-12-04T05:27:02.258206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Start Training!!!","metadata":{}},{"cell_type":"code","source":"def train(model, config, train_loader, valid_loader, device):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n\n    n_epochs, best_loss, step, early_stop_count = config['epochs'], 10000, 0, 0\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n\n        # tqdm is a package to visualize your training progress.\n#         train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_loader:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x) \n            loss = getLoss(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n#             train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n#             train_pbar.set_postfix({'loss': loss.detach().item()})\n#             print(f'Epoch [{epoch+1}/{n_epochs}]')\n#             print(f\"loss: {loss.detach().item()}\")\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = getLoss(pred, y)\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        # Note that step should be called after validate()\n        scheduler.step(mean_valid_loss)\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n            if(epoch % 10 == 0):\n                print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n            \n        if(epoch % 10 == 0):\n            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n            if early_stop_count >= config['early_stop']:\n                print('\\nModel is not improving, so we halt the training session.')\n                print('best loss {:.3f}...'.format(best_loss))\n                return","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.261022Z","iopub.execute_input":"2023-12-04T05:27:02.261305Z","iopub.status.idle":"2023-12-04T05:27:02.284657Z","shell.execute_reply.started":"2023-12-04T05:27:02.261280Z","shell.execute_reply":"2023-12-04T05:27:02.283934Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Inference part","metadata":{}},{"cell_type":"code","source":"def predict(timeseries, test_loader, model, device, output_path, stop_name):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    tots = []\n    titles = []\n\n    window_size = 72 * 2\n#     input_dim = window_size\n#     output_dim = window_size\n    test = timeseries[-window_size:].tolist()\n#     print(f\"test: {test}\")\n\n    i=0\n    for month,day,weekday,hr,mins,lat,lng,act, tot, title in test_loader:\n#         print(i)\n        i+=1\n        tot = tot.tolist()\n        title = list(title)\n        x = torch.FloatTensor(test[-window_size:])\n        x = x.view(-1,len(x))\n        x = x.to(device)      \n        tots = tots + tot\n        titles = titles + title\n        with torch.no_grad():  \n#             model.hidden = (torch.zeros(1,1,model.hidden_dim),\n#                             torch.zeros(1,1,model.hidden_dim))\n            pred = model(x)  \n            test.append(pred.tolist()[-1][-1])\n            preds.append(pred.detach().cpu())\n\n    preds = test[window_size:]\n#     print(f\"preds: {test}\")\n    print(len(preds), len(tots), len(titles))\n    plt.plot(test)\n    plt.show()\n    plt.savefig(stop + \".png\")\n    assert len(preds) == len(tots)\n    assert len(tots) == len(titles)\n    prediction = [['id','sbi']]\n    for (pred, tot, title) in zip(preds, tots, titles):\n        title = title.split('_')\n        real_title = title[0] + '_' + stop + '_' + title[2]\n        prediction.append([real_title, pred*tot])\n    \n    with open(output_path, 'w', newline='') as file:\n    # Step 4: Using csv.writer to write the list to the CSV file\n        writer = csv.writer(file)\n        writer.writerows(prediction) # Use writerows for nested list\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.286955Z","iopub.execute_input":"2023-12-04T05:27:02.287251Z","iopub.status.idle":"2023-12-04T05:27:02.298548Z","shell.execute_reply.started":"2023-12-04T05:27:02.287228Z","shell.execute_reply":"2023-12-04T05:27:02.297807Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Visualize input dataset\n# Iterate through 112 stops\ni=0\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        stop = filename.split('.')[0]\n#         if stop != '500101035': continue\n        config[\"model_name\"] = \"stop\" + stop + \"-RNN-v1.ckpt\"\n        output_path = 'lstm_pred_' + stop + '.csv'\n        print(\"Start training stop \" + stop + \"\\n\")\n        df = pd.read_csv('/kaggle/input/dataset-filled/dataset_filled/' + filename)\n        df_inf = pd.read_csv(\"/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv\")\n        # print(len(df['ratio']))\n        # print(df['ratio'][:50])\n        timeseries = df['ratio'].values.astype('float32') \n        print(timeseries)\n        X_train, y_train, X_valid, y_valid = train_valid_split(timeseries)\n        train_loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=config['batch_size'])\n        valid_loader = DataLoader(TensorDataset(X_valid, y_valid), shuffle=True, batch_size=config['batch_size'])\n#         print(train_loader, valid_loader)\n#         model = UBikeRNN(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        model = UBikeLSTM(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        train(model, config, train_loader, valid_loader, device)\n        print(\"Start predicting stop \" + stop + \"\\n\")\n        test_data = df_inf.values.tolist()\n        print(len(test_data))\n        test_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n        print(len(test_loader))\n        predict(timeseries, test_loader, model, device, output_path, stop_name=stop)\n        print(\"Finish predicting stop \" + stop + \"\\n\")\n        i+=1\n#         if i == 3:\n#             break\n        print(f\"Prgress {i}/112\\n\")\n# plt.plot(timeseries)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T05:27:02.299721Z","iopub.execute_input":"2023-12-04T05:27:02.300012Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start training stop 500101216\n\n[1.      1.      1.      ... 0.95833 0.95833 0.95833]\n2734 1200\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/778121417.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  return torch.tensor(X), torch.tensor(y)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2590, 144]) torch.Size([2590, 144])\ntorch.Size([1056, 144]) torch.Size([1056, 144])\nSaving model with loss 0.247...\nEpoch [1/500]: Train loss: 0.2768, Valid loss: 0.2465\n","output_type":"stream"}]}]}