{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-09T13:10:01.079096Z","iopub.status.busy":"2023-12-09T13:10:01.078716Z","iopub.status.idle":"2023-12-09T13:10:01.091316Z","shell.execute_reply":"2023-12-09T13:10:01.090372Z","shell.execute_reply.started":"2023-12-09T13:10:01.079070Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import csv\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.097856Z","iopub.status.busy":"2023-12-09T13:10:01.097345Z","iopub.status.idle":"2023-12-09T13:10:01.102465Z","shell.execute_reply":"2023-12-09T13:10:01.101466Z","shell.execute_reply.started":"2023-12-09T13:10:01.097822Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts"]},{"cell_type":"markdown","metadata":{},"source":["## Define all paths"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.116180Z","iopub.status.busy":"2023-12-09T13:10:01.115831Z","iopub.status.idle":"2023-12-09T13:10:01.121131Z","shell.execute_reply":"2023-12-09T13:10:01.120146Z","shell.execute_reply.started":"2023-12-09T13:10:01.116144Z"},"trusted":true},"outputs":[],"source":["data_path_to1203 = \"/kaggle/input/filled-csv-to1203/filled_csv_to1203.csv\"\n","inference_path_1021_1024 = \"/kaggle/input/allstops-inf-1021-1024/allstops_inf_1021_to_1024.csv\"\n","inference_path_1204_1210 = \"/kaggle/input/allstops-inf-1204-1210/allstops_inf_1204_to_1210.csv\"\n","inference_path_1211_1217 = \"/kaggle/input/allstops-inf-1211-1217/allstops_inf_1211_to_1217.csv\"\n","output_path_to1203 = \"predictions_lstm_full.csv\"\n","ckpt_path = \"/kaggle/working/lstm_v10_to1203.ckpt\""]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing\n","### - configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.138727Z","iopub.status.busy":"2023-12-09T13:10:01.138383Z","iopub.status.idle":"2023-12-09T13:10:01.144578Z","shell.execute_reply":"2023-12-09T13:10:01.143498Z","shell.execute_reply.started":"2023-12-09T13:10:01.138699Z"},"trusted":true},"outputs":[],"source":["total_stops = 112\n","total_days = 55    # 10/2 ~ 12/3\n","valid_days = 14    # 11/20 ~ 12/3\n","day_period = 72\n","stop_period = total_days * day_period\n","lookback = 2 * day_period  # 2 days\n","lookforward = 1 * day_period # 1 time\n","idx = {'10/11_23:40': 719,'10/16_00:00': 720,'10/19_00:00': 936,'10/20_23:40': 1079,'10/25_00:00': 1080,'11/18_23:40': 2879,'12/02_00:00': 3816,'12/03_23:40': 3959,'12/07_00:00': 4176}\n","# print(idx['10/11_23:40'])\n","# datafile = pd.read_csv(data_path_to1203)\n","# features_timeseries = datafile[[c for c in datafile.columns if c in ['weekday','hr','min','lat','lng','act','ratio']]].values.astype('float32')\n","# print(features_timeseries[idx['11/18_23:40']])"]},{"cell_type":"markdown","metadata":{},"source":["### - define functions"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","7\n","8\n","9\n"]}],"source":["for i in range(10):\n","    if i >5 and i<7:\n","        continue\n","    print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.158210Z","iopub.status.busy":"2023-12-09T13:10:01.157929Z","iopub.status.idle":"2023-12-09T13:10:01.164719Z","shell.execute_reply":"2023-12-09T13:10:01.163756Z","shell.execute_reply.started":"2023-12-09T13:10:01.158185Z"},"trusted":true},"outputs":[],"source":["def create_dataset(training_data):\n","    \"\"\"Transform a time series into a prediction dataset, from lookback predict lookforward\n","    \n","    Args:\n","        dataset: A numpy array of time series, first dimension is the time steps\n","        lookback: Size of window for prediction\n","    \"\"\"\n","    X, Y = [], []\n","\n","    for stop in range(total_stops):\n","        for i in range(len(training_data[stop]) - (lookback + lookforward)):\n","            if i + lookback + lookforward > (stop+1)*idx['10/11_23:40'] and i < (stop+1)*idx['10/16_00:00']:\n","                continue\n","            if i + lookback + lookforward > (stop+1)*idx['10/20_23:40'] and i < (stop+1)*idx['10/25_00:00']:\n","                continue\n","            x = training_data[stop][i:i + lookback]\n","            y = training_data[stop][i + lookback:i + lookback + lookforward][:,-1] # ratio at [:,-1]\n","            X.append(x)\n","            Y.append(y)\n","        \n","    return torch.tensor(np.array(X)), torch.tensor(np.array(Y))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.184868Z","iopub.status.busy":"2023-12-09T13:10:01.184343Z","iopub.status.idle":"2023-12-09T13:10:01.192011Z","shell.execute_reply":"2023-12-09T13:10:01.190963Z","shell.execute_reply.started":"2023-12-09T13:10:01.184841Z"},"trusted":true},"outputs":[],"source":["def train_valid_split(timeseries):\n","    # train-valid split with 10/2~11/19, 11/20~12/03 (2 weeks)\n","    train_size = (total_days - valid_days) * day_period # 2952\n","    valid_size = valid_days * day_period\n","    train = []\n","    valid = []\n","    for stop in range(total_stops):\n","        train.append(timeseries[stop * stop_period : stop * stop_period + train_size])\n","        valid.append(timeseries[stop * stop_period + train_size : stop * (stop_period) + stop_period])\n","    print(f\"train.shape = {np.array(train).shape}, valid.shape = {np.array(valid).shape}\")\n","\n","    X_train, Y_train = create_dataset(train)\n","    X_valid, Y_valid = create_dataset(valid)\n","\n","    print(\"X.shape = [batch, seq, features], Y.shape = [batch, seq]\")\n","#     print(Y_train[0])\n","    print(f\"X_train.shape = {X_train.shape}, Y_train.shape = {Y_train.shape}\")\n","    print(f\"X_valid.shape = {X_valid.shape}, Y_valid.shape = {Y_valid.shape}\")\n","    return X_train, Y_train, X_valid, Y_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.217137Z","iopub.status.busy":"2023-12-09T13:10:01.216543Z","iopub.status.idle":"2023-12-09T13:10:01.221229Z","shell.execute_reply":"2023-12-09T13:10:01.220309Z","shell.execute_reply.started":"2023-12-09T13:10:01.217105Z"},"trusted":true},"outputs":[],"source":["# datafile = pd.read_csv(data_path_to1203)\n","# features_timeseries = datafile[[c for c in datafile.columns if c in ['weekday','hr','min','lat','lng','act','ratio']]].values.astype('float32')\n","# print(f\"origin_data.shape = {features_timeseries.shape}\")\n","# X_train, y_train, X_valid, y_valid = train_valid_split(features_timeseries)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.240733Z","iopub.status.busy":"2023-12-09T13:10:01.240000Z","iopub.status.idle":"2023-12-09T13:10:01.247009Z","shell.execute_reply":"2023-12-09T13:10:01.245991Z","shell.execute_reply.started":"2023-12-09T13:10:01.240704Z"},"trusted":true},"outputs":[],"source":["def read_full_dataset(data_path, batch_size):\n","    datafile = pd.read_csv(data_path)\n","    features_timeseries = datafile[[c for c in datafile.columns if c in ['weekday','hr','min','lat','lng','act','ratio']]].values.astype('float32')\n","    print(f\"origin_data.shape = {features_timeseries.shape}\")\n","    X_train, Y_train, X_valid, Y_valid = train_valid_split(features_timeseries)\n","    train_loader = DataLoader(TensorDataset(X_train, Y_train), shuffle=False, batch_size=batch_size)\n","    valid_loader = DataLoader(TensorDataset(X_valid, Y_valid), shuffle=False, batch_size=batch_size)\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.283390Z","iopub.status.busy":"2023-12-09T13:10:01.282719Z","iopub.status.idle":"2023-12-09T13:10:01.289093Z","shell.execute_reply":"2023-12-09T13:10:01.288161Z","shell.execute_reply.started":"2023-12-09T13:10:01.283359Z"},"trusted":true},"outputs":[],"source":["def read_inf_dataset(inf_path, batch_size):\n","    df_inf = pd.read_csv(inf_path)\n","    inf_timeseries = df_inf[[c for c in df_inf.columns if c in ['month','day','weekday','hr','min','lat','lng','act','tot','title']]].values.tolist()\n","    print(f\"inf_timeseries.shape = {len(inf_timeseries)}\")\n","    test_loader = DataLoader(inf_timeseries, shuffle=False, batch_size=batch_size)\n","    return test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.299517Z","iopub.status.busy":"2023-12-09T13:10:01.299216Z","iopub.status.idle":"2023-12-09T13:10:01.306518Z","shell.execute_reply":"2023-12-09T13:10:01.305691Z","shell.execute_reply.started":"2023-12-09T13:10:01.299493Z"},"trusted":true},"outputs":[],"source":["def get_test_dataset(period):\n","    test = []\n","    datafile = pd.read_csv(data_path_to1203)\n","    features_timeseries = datafile[[c for c in datafile.columns if c in ['weekday','hr','min','lat','lng','act','ratio']]].values.astype('float32')\n","    \n","    if period == '1021-1024':\n","        # fetch data from 1019 to 1020\n","        for stop in range(total_stops):\n","            test.append(features_timeseries[stop * stop_period + idx['10/19_00:00']: stop * stop_period + idx['10/20_23:40'] + 1])\n","    elif period == '1204-1210':\n","        # fetch data from 1202 to 1203\n","        for stop in range(total_stops):\n","            test.append(features_timeseries[stop * stop_period + idx['12/02_00:00']: stop * stop_period + idx['12/03_23:40'] + 1])\n","    print(f\"test_dataset.shape = {np.array(test).shape}\")\n","    return np.array(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.313147Z","iopub.status.busy":"2023-12-09T13:10:01.312433Z","iopub.status.idle":"2023-12-09T13:10:01.318525Z","shell.execute_reply":"2023-12-09T13:10:01.317537Z","shell.execute_reply.started":"2023-12-09T13:10:01.313110Z"},"trusted":true},"outputs":[],"source":["def shift_test_dataset(test_dataset, new_data):\n","    \"\"\"Shift the data to the next time\n","    \n","    Args:\n","        dataset: A numpy array of time series, size = 2 days\n","        new data: New prediction append to dataset to shift window\n","    \"\"\"\n","    X = test_dataset\n","    if new_data != None:\n","        X = np.concatenate((test_dataset, np.array(new_data)), axis=1)\n","        X = X[:,1:,:]\n","    return torch.tensor(np.array(X))"]},{"cell_type":"markdown","metadata":{},"source":["## Build a basic LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.326599Z","iopub.status.busy":"2023-12-09T13:10:01.326300Z","iopub.status.idle":"2023-12-09T13:10:01.336516Z","shell.execute_reply":"2023-12-09T13:10:01.335616Z","shell.execute_reply.started":"2023-12-09T13:10:01.326574Z"},"trusted":true},"outputs":[],"source":["class UBikeLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n","        super().__init__()\n","        \n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = layer_dim\n","        self.output_size = output_dim\n","        \n","        # initialize LSTM   \n","        self.block = nn.Sequential(\n","            nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True, bidirectional=False),\n","            nn.Linear(hidden_dim, output_dim),\n","            nn.Sigmoid(),\n","        )\n","        \n","        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True, bidirectional=False)\n","        # the output of LSTM is classified by linear and sigmoid functions\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        h_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # hidden state\n","        c_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # internal state\n","        lstm_out, (hn, cn)  = self.lstm(x)\n","#         print(f\"lstm_out.shape = {lstm_out.shape}\") # [28, 144, 128] (N=batch size,L=seq,H=hidden) \n","        lstm_out = lstm_out[:,-1,:]\n","        out = self.linear(lstm_out) # [32,hidden=128] to [32,output=72]\n","        out = self.sigmoid(out)\n","        \n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.347871Z","iopub.status.busy":"2023-12-09T13:10:01.347550Z","iopub.status.idle":"2023-12-09T13:10:01.353192Z","shell.execute_reply":"2023-12-09T13:10:01.352302Z","shell.execute_reply.started":"2023-12-09T13:10:01.347845Z"},"trusted":true},"outputs":[],"source":["def getLoss(pred, label):\n","#     loss = torch.mean(3 * torch.abs(pred-label) * (torch.abs(pred-1/3) + torch.abs(pred-2/3)))\n","    loss = torch.mean(torch.square(pred-label))\n","#     print(f\"loss = {loss}\")\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["## Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.377350Z","iopub.status.busy":"2023-12-09T13:10:01.376976Z","iopub.status.idle":"2023-12-09T13:10:01.383365Z","shell.execute_reply":"2023-12-09T13:10:01.382393Z","shell.execute_reply.started":"2023-12-09T13:10:01.377323Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"batch_size\": 28,\n","    \"epochs\": 50,\n","    \"learning_rate\": 1e-4,\n","    \"weight_decay\": 5e-3,\n","    \"save_dir\": \"/kaggle/working/\",\n","    \"model_name\": \"lstm_v10_to1203.ckpt\",\n","    \"early_stop\": 150,\n","}\n","\n","# model parameters\n","window_size = lookback\n","input_dim =  7 # number of features !!!\n","hidden_dim = 128   # the hidden dim\n","layer_dim = 3   # the number of hidden layers\n","output_dim = lookforward\n","drop_prob = 0.2\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["## Training Part"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.406898Z","iopub.status.busy":"2023-12-09T13:10:01.406195Z","iopub.status.idle":"2023-12-09T13:10:01.426079Z","shell.execute_reply":"2023-12-09T13:10:01.425178Z","shell.execute_reply.started":"2023-12-09T13:10:01.406871Z"},"trusted":true},"outputs":[],"source":["def train(model, config, train_loader, valid_loader, device):\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n","#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n","    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=1,T_mult=2)\n","\n","    n_epochs, best_loss, step, early_stop_count = config['epochs'], 10000, 0, 0\n","    actual_ratio = [[] for i in range(total_stops*2)]\n","    pred_ratio = [[] for i in range(total_stops*2)]\n","    mse_loss_train = []\n","    mse_loss_val = []\n","    for epoch in range(n_epochs):\n","        model.train() # Set your model to train mode.\n","        loss_record = []\n","\n","        # tqdm is a package to visualize your training progress.\n","        train_pbar = tqdm(train_loader, position=0, leave=True)\n","\n","        for x, y in train_pbar:\n","            optimizer.zero_grad()               # Set gradient to zero.\n","            x, y = x.to(device), y.to(device)   # Move your data to device. \n","            pred = model(x) \n","#             print(f\"pred.shape = {pred.shape}, y.shape = {y.shape}\") # [32,72]\n","#             print(f\"pred = {pred.detach().cpu()[0][0]}, y = {y[0][0]}\")\n","            loss = getLoss(pred, y)\n","            loss.backward()                     # Compute gradient(backpropagation).\n","            optimizer.step()                    # Update parameters.\n","            step += 1\n","            loss_record.append(loss.detach().item())\n","            \n","            # Display current epoch number and loss on tqdm progress bar.\n","            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n","            train_pbar.set_postfix({'loss': loss.detach().item()})\n","        \n","        scheduler.step()\n","        mean_train_loss = sum(loss_record)/len(loss_record)\n","        mse_loss_train.append(mean_train_loss)\n","        print(len(mse_loss_train))\n","        \n","        model.eval() # Set your model to evaluation mode.\n","        loss_record = []\n","        count = 0\n","        stop = 0\n","        temp = []\n","        for x, y in valid_loader:\n","            x, y = x.to(device), y.to(device)\n","            with torch.no_grad():\n","                pred = model(x)\n","                print(f\"pred = {pred}, y = {y}\")\n","                loss = getLoss(pred, y)\n","            \n","            if epoch == n_epochs - 1:\n","                temp.append(stop)\n","#                 print(f\"count={count},stop={stop}\")\n","                actual_ratio[stop].append(y[0][0].item())\n","                pred_ratio[stop].append(pred.detach().cpu()[0][0])\n","                if (count+1) % (2952 - 145) == 0:\n","                    stop += 1\n","                count += 1\n","                \n","            loss_record.append(loss.item())\n","        \n","        print(len(temp))\n","        if epoch == n_epochs - 1:\n","            plt.plot(actual_ratio[0])\n","            plt.plot(pred_ratio[0])\n","            plt.legend(['actual', 'predict'])\n","            plt.title(\"Train / Valid\")\n","#             plt.savefig(\"Train/Valid.png\")\n","            plt.show()\n","            \n","        mean_valid_loss = sum(loss_record)/len(loss_record)\n","        mse_loss_val.append(mean_valid_loss)\n","        \n","        # Note that step should be called after validate() when applying ReduceLROnPlateau sheduler\n","#         scheduler.step(mean_valid_loss)\n","        \n","        if mean_valid_loss < best_loss:\n","            best_loss = mean_valid_loss\n","            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n","            if(epoch % 10 == 0):\n","                print('Saving model with loss {:.3f}...'.format(best_loss))\n","            early_stop_count = 0\n","        else: \n","            early_stop_count += 1\n","            \n","        if(epoch % 10 == 0):\n","            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n","        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n","\n","            if early_stop_count >= config['early_stop']:\n","                print('\\nModel is not improving, so we halt the training session.')\n","                print('best loss {:.3f}...'.format(best_loss))\n","                return\n","            \n","    plt.plot(mse_loss_train)\n","    plt.plot(mse_loss_val)\n","    plt.title(\"MSE loss = {:.3f}\".format(best_loss))\n","    plt.legend(['train', 'valid'])\n","#     plt.savefig('mse_loss.png')\n","#     plt.show()\n","            "]},{"cell_type":"markdown","metadata":{},"source":["## Inference part"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.445220Z","iopub.status.busy":"2023-12-09T13:10:01.444918Z","iopub.status.idle":"2023-12-09T13:10:01.463271Z","shell.execute_reply":"2023-12-09T13:10:01.462291Z","shell.execute_reply.started":"2023-12-09T13:10:01.445194Z"},"trusted":true},"outputs":[],"source":["def predict(test_dataset, test_loader, model, device, output_path):\n","    model.eval() # Set your model to evaluation mode.\n","    preds = []\n","    tots = []\n","    titles = []\n","    ratio = test_dataset[:,:,-1]\n","    ratio_list = test_dataset[:,:,-1].tolist()\n","    pred_112_504 = []\n","    batch_size = config['batch_size']\n","    month = 12\n","    for i, (month,day,weekday,hr,min,lat,lng,act,tot,title) in enumerate(test_loader):\n","        ratio_b = ratio[(i%4)*batch_size:((i%4)+1)*batch_size,:] # [28,144,1]\n","        # new_data.shape = [112,1,1]\n","        stack = torch.hstack((weekday.unsqueeze(1),hr.unsqueeze(1),min.unsqueeze(1),lat.unsqueeze(1),lng.unsqueeze(1),act.unsqueeze(1)))\n","#         print(torch.FloatTensor(test_dataset[(i%4)*batch_size:((i%4)+1)*batch_size,-1,-1]).shape)\n","        \n","        test_dataset_b = torch.FloatTensor((test_dataset[(i%4)*batch_size:((i%4)+1)*batch_size,:,:]))\n","#         print(f\"test_dataset_b.shape = {test_dataset_b.shape}\")\n","        \n","        month = month[0].item()\n","        tot = tot.tolist()\n","        title = list(title)\n","        \n","        if i == 0:\n","            x = test_dataset_b\n","        \n","#         print(f\"x.shape={x.shape}\") # [112, 144, 7]\n","        x = x.to(torch.float32)\n","        x = x.to(device)      \n","        tots = tots + tot\n","        titles = titles + title\n","        \n","        with torch.no_grad():  \n","            pred = model(x)   # tensor\n","#             print(len(pred.tolist())) # (28,1)\n","            \n","            if i % 4 != 0:\n","                pred_112_1 = torch.vstack((pred_112_1, pred))\n","                if i==503:\n","                    pred_112_504.append(pred_112_1.tolist())\n","            else:\n","                if i != 0:\n","                    pred_112_504.append(pred_112_1.tolist())\n","                pred_112_1 = pred\n","\n","            new_data = torch.hstack((stack,torch.FloatTensor(pred.detach().cpu())))\n","#             print(f\"new_data.shape = {np.array(new_data).shape}\") # (28,7)\n","            \n","            x = shift_test_dataset(test_dataset_b, new_data.reshape(28,1,7))\n","        \n","    print(f'pred_112_504 shape: {np.array(pred_112_504).shape}') # [504,112,1]\n","    preds = torch.Tensor(pred_112_504).permute(1,0,2).squeeze(2) # (112,504)\n","    print(len(preds), len(tots), len(titles))\n","    print(f\"preds shape = {np.array(preds).shape}\")\n","    print(f\"preds[0] = {preds[0]}\")\n","    \n","#     plt.plot(ratio_list)\n","#     plt.title(\"Predictions\")\n","#     plt.show()\n","#     plt.close()\n","    \n","    plt.plot(preds[0].tolist()[:150])\n","    plt.title(\"pred_stop_001_1204_1210\" +\".png\")\n","#     plt.savefig(stop + '_' + str(month) +\".png\")\n","    plt.show()\n","    plt.close()\n","    \n","    preds_56448_1 = []\n","    for i in range(len(preds[0])):\n","        for j in range(len(preds)):\n","            preds_56448_1.append(preds[j][i].item())\n","            \n","#     print(preds_56448_1)\n","    \n","    print(f\"len(preds_56448_1) = {len(preds_56448_1)}, len(tots) = {len(tots)}\")\n","    assert len(preds_56448_1) == len(tots)\n","    assert len(tots) == len(titles)\n","    prediction = []\n","    \n","    if month == 12:\n","        prediction = [['id','sbi']]\n","    for (pred, tot, title) in zip(preds_56448_1, tots, titles):\n","#         title = title.split('_')\n","#         real_title = title[0] + '_' + stop + '_' + title[2]\n","        prediction.append([title, pred*tot])\n","    \n","    with open(output_path, 'a', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerows(prediction) # Use writerows for nested list\n","        \n","    return"]},{"cell_type":"markdown","metadata":{},"source":["## Start training and predicting !!!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T13:10:01.467702Z","iopub.status.busy":"2023-12-09T13:10:01.467417Z"},"trusted":true},"outputs":[],"source":["# Assign model and start training...\n","model = UBikeLSTM(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n","train_loader, valid_loader = read_full_dataset(data_path_to1203, batch_size=config['batch_size'])\n","train(model, config, train_loader, valid_loader, device)\n","\n","# checkpoint = torch.load(\"/kaggle/input/better-ckpt/lstm_v10_to1203 (2).ckpt\")\n","# model.load_state_dict(checkpoint)\n","\n","print(\"Start predicting from 1204 to 1210...\")\n","\n","test_loader = read_inf_dataset(inference_path_1204_1210, batch_size=config['batch_size'])\n","test_dataset = get_test_dataset(period='1204-1210')\n","predict(test_dataset, test_loader, model, device, output_path=output_path_to1203)\n","\n","print(\"Start predicting from 1021 to 1024...\")\n","\n","test_loader = read_inf_dataset(inference_path_1021_1024, batch_size=config['batch_size'])\n","test_dataset = get_test_dataset(period='1021-1024')\n","predict(test_dataset, test_loader, model, device, output_path=output_path_to1203)\n","        \n","print(\"Finish prediction\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4132428,"sourceId":7155788,"sourceType":"datasetVersion"},{"datasetId":4132433,"sourceId":7155793,"sourceType":"datasetVersion"},{"datasetId":4132441,"sourceId":7155802,"sourceType":"datasetVersion"},{"datasetId":4132509,"sourceId":7155885,"sourceType":"datasetVersion"},{"datasetId":4133439,"sourceId":7157130,"sourceType":"datasetVersion"},{"datasetId":4136278,"sourceId":7161317,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
