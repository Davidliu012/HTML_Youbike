{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7106821,"sourceType":"datasetVersion","datasetId":4097269},{"sourceId":7107818,"sourceType":"datasetVersion","datasetId":4097960},{"sourceId":7111750,"sourceType":"datasetVersion","datasetId":4100724},{"sourceId":7112592,"sourceType":"datasetVersion","datasetId":4101363},{"sourceId":7122347,"sourceType":"datasetVersion","datasetId":4108260},{"sourceId":7122608,"sourceType":"datasetVersion","datasetId":4108448},{"sourceId":7122630,"sourceType":"datasetVersion","datasetId":4108458},{"sourceId":7122632,"sourceType":"datasetVersion","datasetId":4108460}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T06:28:28.385025Z","iopub.execute_input":"2023-12-07T06:28:28.385435Z","iopub.status.idle":"2023-12-07T06:28:28.813858Z","shell.execute_reply.started":"2023-12-07T06:28:28.385405Z","shell.execute_reply":"2023-12-07T06:28:28.812849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:28.816538Z","iopub.execute_input":"2023-12-07T06:28:28.817394Z","iopub.status.idle":"2023-12-07T06:28:30.887483Z","shell.execute_reply.started":"2023-12-07T06:28:28.817359Z","shell.execute_reply":"2023-12-07T06:28:30.886575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(dataset, lookback):\n    \"\"\"Transform a time series into a prediction dataset\n    \n    Args:\n        dataset: A numpy array of time series, first dimension is the time steps\n        lookback: Size of window for prediction\n    \"\"\"\n    X, y = [], []\n    one_day_period = 72\n    for i in range(len(dataset)-(lookback+one_day_period)):\n        feature = dataset[i:i+lookback]\n        target = dataset[i+lookback:i+lookback+one_day_period][:,-1] # ratio at [:,-1]\n        X.append(feature)\n        y.append(target)\n    return torch.tensor(np.array(X)), torch.tensor(np.array(y))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.888768Z","iopub.execute_input":"2023-12-07T06:28:30.889260Z","iopub.status.idle":"2023-12-07T06:28:30.896273Z","shell.execute_reply.started":"2023-12-07T06:28:30.889229Z","shell.execute_reply":"2023-12-07T06:28:30.895385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_valid_split(timeseries):\n    # train-valid split with 10/2~11/16 [0:2734], 11/17~11/30[2734:]\n    # 10/2 ~ 10/11 [:711]\n    train_size = 2734\n    valid_size = len(timeseries) - train_size\n    train, valid = timeseries[:train_size], timeseries[train_size:]\n    print(len(train), len(valid))\n    # window size approximately = 3 days\n    lookback = 72 * 2\n    X_train, y_train = create_dataset(train, lookback=lookback)\n    X_valid, y_valid = create_dataset(valid, lookback=lookback)\n    # X_test, y_test = create_dataset(test, lookback=lookback)\n    print(\"X.shape = [batch, seq, features], y.shape = [batch, seq]\")\n#     print(y_train[0])\n    print(X_train.shape, y_train.shape)\n    print(X_valid.shape, y_valid.shape)\n    return X_train, y_train, X_valid, y_valid","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.897522Z","iopub.execute_input":"2023-12-07T06:28:30.897876Z","iopub.status.idle":"2023-12-07T06:28:30.907901Z","shell.execute_reply.started":"2023-12-07T06:28:30.897845Z","shell.execute_reply":"2023-12-07T06:28:30.907102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_dataset(dataset, new_data):\n    \"\"\"\n     Args:\n        dataset: A numpy array of time series, size = 144\n        new data: New prediction append to dataset to shift window\n    \"\"\"\n    X = []\n    if new_data == None:\n        X = dataset\n    else:\n        X = np.append(dataset, [np.array(new_data)],axis=0)\n        X = X[1:]\n    return torch.tensor(np.array(X))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.910707Z","iopub.execute_input":"2023-12-07T06:28:30.911063Z","iopub.status.idle":"2023-12-07T06:28:30.922915Z","shell.execute_reply.started":"2023-12-07T06:28:30.911014Z","shell.execute_reply":"2023-12-07T06:28:30.921915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a basic RNN model","metadata":{}},{"cell_type":"code","source":"class UBikeRNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True)    \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        # h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n#         print(x.size(0)) # x.size(0) = batch size\n        hidden = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n        x = x.view(len(x),1,-1)\n#         print(x.shape)\n        # Passing in the input and hidden state into the model and obtaining outputs\n        out, hidden = self.rnn(x, hidden)\n        \n        # Reshaping the outputs such that it can be fit into the fully connected layer\n        out = out.contiguous().view(-1, self.hidden_dim)\n        out = self.fc(out)\n        out = self.sig(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.924080Z","iopub.execute_input":"2023-12-07T06:28:30.924416Z","iopub.status.idle":"2023-12-07T06:28:30.935396Z","shell.execute_reply.started":"2023-12-07T06:28:30.924383Z","shell.execute_reply":"2023-12-07T06:28:30.934374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBikeLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = layer_dim\n        self.output_size = output_dim\n        \n        # initialize LSTM   \n        self.block = nn.Sequential(\n            nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True, bidirectional=False),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Sigmoid(),\n        )\n        \n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True, bidirectional=False)\n        # the output of LSTM is classified by linear and sigmoid functions\n        self.linear = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        h_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # hidden state\n        c_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # internal state\n        lstm_out, (hn, cn)  = self.lstm(x)\n#         hn = hn.view(-1, self.hidden_dim) # reshaping the data for Dense layer next\n#         print(f\"lstm_out.shape = {lstm_out.shape}\") # [32, 144, 128] (N=batch size,L=seq,H=hidden) \n#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        lstm_out = lstm_out[:,-1,:]\n        out = self.linear(lstm_out) # [32,hidden=128] to [32,output=72]\n        out = self.sigmoid(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.936529Z","iopub.execute_input":"2023-12-07T06:28:30.936805Z","iopub.status.idle":"2023-12-07T06:28:30.951174Z","shell.execute_reply.started":"2023-12-07T06:28:30.936782Z","shell.execute_reply":"2023-12-07T06:28:30.949938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"code","source":"def getLoss(pred, label):\n#     loss = torch.mean(3 * torch.abs(pred-label) * (torch.abs(pred-1/3) + torch.abs(pred-2/3)))\n    loss = torch.mean(torch.square(pred-label))\n#     print(f\"loss = {loss}\")\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.952614Z","iopub.execute_input":"2023-12-07T06:28:30.953347Z","iopub.status.idle":"2023-12-07T06:28:30.963834Z","shell.execute_reply.started":"2023-12-07T06:28:30.953291Z","shell.execute_reply":"2023-12-07T06:28:30.963052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 32,\n    \"data_filepath\": '/kaggle/input/stop001-sorted/500101001_sorted.csv',\n    \"inference_filepath\": '/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv',\n    \"epochs\": 150,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 5e-3,\n    \"save_dir\": \"/kaggle/working/\",\n    \"model_name\": \"stop001-RNN-v1.ckpt\",\n    \"early_stop\": 150,\n}\n# model parameters\nwindow_size = 72 * 2\ninput_dim =  5 # number of features !!!\nhidden_dim = 128   # the hidden dim\nlayer_dim = 3   # the number of hidden layers\noutput_dim = 72\ndrop_prob = 0.2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:30.964835Z","iopub.execute_input":"2023-12-07T06:28:30.965134Z","iopub.status.idle":"2023-12-07T06:28:30.998925Z","shell.execute_reply.started":"2023-12-07T06:28:30.965111Z","shell.execute_reply":"2023-12-07T06:28:30.998008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training!!!","metadata":{}},{"cell_type":"code","source":"def train(model, config, train_loader, valid_loader, device):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=1,T_mult=2)\n\n    n_epochs, best_loss, step, early_stop_count = config['epochs'], 10000, 0, 0\n    actual_ratio = []\n    pred_ratio = []\n    mse_loss_train = []\n    mse_loss_val = []\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n\n        # tqdm is a package to visualize your training progress.\n#         train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_loader:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x) \n#             print(f\"pred.shape = {pred.shape}, y.shape = {y.shape}\") # [32,72]\n#             print(f\"pred = {pred.detach().cpu()[0][0]}, y = {y[0][0]}\")\n            loss = getLoss(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n#             train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n#             train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n        mse_loss_train.append(mean_train_loss)\n        \n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = getLoss(pred, y)\n            \n            if epoch == n_epochs - 1:\n                actual_ratio.append(y[0][0].item())\n                pred_ratio.append(pred.detach().cpu()[0][0])\n            loss_record.append(loss.item())\n        \n        if epoch == n_epochs - 1:\n            plt.plot(actual_ratio)\n            plt.plot(pred_ratio)\n            # plt.title('MAE:{:.3f}, MAPE:{:.3f}'.format(mae, mape))\n            plt.legend(['actual', 'predict'])\n            plt.title(\"Train / Valid\")\n            plt.savefig(\"Train/Valid.png\")\n#             plt.show()\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        mse_loss_val.append(mean_valid_loss)\n        # Note that step should be called after validate()\n        scheduler.step(mean_valid_loss)\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n            if(epoch % 10 == 0):\n                print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n            \n        if(epoch % 10 == 0):\n            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n            if early_stop_count >= config['early_stop']:\n                print('\\nModel is not improving, so we halt the training session.')\n                print('best loss {:.3f}...'.format(best_loss))\n                return\n            \n    plt.plot(mse_loss_train)\n    plt.plot(mse_loss_val)\n    plt.title(\"MSE loss = {:.3f}\".format(best_loss))\n    plt.legend(['train', 'valid'])\n    plt.savefig('mse_loss.png')\n    plt.show()\n            ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:31.000204Z","iopub.execute_input":"2023-12-07T06:28:31.000504Z","iopub.status.idle":"2023-12-07T06:28:31.029470Z","shell.execute_reply.started":"2023-12-07T06:28:31.000478Z","shell.execute_reply":"2023-12-07T06:28:31.028638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference part","metadata":{}},{"cell_type":"code","source":"def predict(test_dataset, test_loader, model, device, output_path, stop_name):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    tots = []\n    titles = []\n    ratio_list = test_dataset[:,-1].tolist()\n#     test_dataset = timeseries[-window_size:]   # last 144 (2 days) dataset\n#     print(np.array(test_dataset).shape) # (144, 7)\n\n    i=0\n    month = 12\n    for month,day,weekday,hr,min,act,tot,title in test_loader:\n        month = month.item()\n        tot = tot.tolist()\n        title = list(title)\n        \n        if i == 0:\n            x = create_test_dataset(test_dataset, None)\n        else: \n            x = create_test_dataset(test_dataset, [weekday.item(),hr.item(),min.item(),act.item(),ratio_list[-1]])\n        test_dataset = x\n        x = x.unsqueeze(0)\n#         print(f\"x.shape={x.shape}\") # [1, 144, 7]\n        x = x.to(torch.float32)\n        x = x.to(device)      \n        tots = tots + tot\n        titles = titles + title\n        with torch.no_grad():  \n            pred = model(x)  \n#             print(f\"pred.tolist()[-1][-1] = {pred.tolist()[-1][-1]}\")\n            ratio_list.append(pred.detach().cpu()[0][0].item())\n            preds.append(pred.detach().cpu())\n        i+=1\n    preds = ratio_list[window_size:]\n    print(len(preds), len(tots), len(titles))\n    \n    plt.plot(ratio_list)\n    plt.title(\"Predictions\")\n    plt.show()\n    plt.close()\n    \n    plt.plot(preds)\n    plt.title(\"pred_stop_\" + stop + \"_1204_1210\" +\".png\")\n    plt.savefig(stop + '_' + str(month) +\".png\")\n    plt.close()\n    assert len(preds) == len(tots)\n    assert len(tots) == len(titles)\n    prediction = []\n    if month == 12:\n        prediction = [['id','sbi']]\n    for (pred, tot, title) in zip(preds, tots, titles):\n        title = title.split('_')\n        real_title = title[0] + '_' + stop + '_' + title[2]\n#         print(pred, tot, pred*tot)\n        prediction.append([real_title, pred*tot])\n    \n    with open(output_path, 'a', newline='') as file:\n    # Step 4: Using csv.writer to write the list to the CSV file\n        writer = csv.writer(file)\n        writer.writerows(prediction) # Use writerows for nested list\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:31.030916Z","iopub.execute_input":"2023-12-07T06:28:31.031291Z","iopub.status.idle":"2023-12-07T06:28:31.046270Z","shell.execute_reply.started":"2023-12-07T06:28:31.031258Z","shell.execute_reply":"2023-12-07T06:28:31.045314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running ...","metadata":{}},{"cell_type":"code","source":"# Visualize input dataset\n# Iterate through 112 stops\ni=0\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        stop = filename.split('.')[0]\n        if stop != '500101216': continue\n        config[\"model_name\"] = \"stop-\" + stop + \"-RNN-v1.ckpt\"\n        output_path = 'lstm_pred_' + stop + '.csv'\n        print(\"Start training stop \" + stop + \"\\n\")\n        df = pd.read_csv('/kaggle/input/dataset-filled/dataset_filled/' + filename)\n        df_inf = pd.read_csv(\"/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv\")\n        df_inf2 = pd.read_csv(\"/kaggle/input/stop001-1021-1024-inf/stop001_inf_1021_to_1024.csv\")\n        \n        timeseries = df[[c for c in df.columns if c in ['weekday','hr','min','act','ratio']]].values.astype('float32')\n        print(timeseries.shape)\n        X_train, y_train, X_valid, y_valid = train_valid_split(timeseries)\n        train_loader = DataLoader(TensorDataset(X_train, y_train), shuffle=False, batch_size=config['batch_size'])\n        valid_loader = DataLoader(TensorDataset(X_valid, y_valid), shuffle=False, batch_size=config['batch_size'])\n#         print(train_loader, valid_loader)\n#         model = UBikeRNN(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        model = UBikeLSTM(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        train(model, config, train_loader, valid_loader, device)\n        \n        print(\"Start predicting stop \" + stop + \"\\n\")\n        print(\"from 1204 to 1210...\")\n        inf_data = df_inf[[c for c in df.columns if c in ['month','day','weekday','hr','min','act','tot','title']]].values.tolist()\n        test_loader = DataLoader(inf_data, shuffle=False, batch_size=1)\n        test_dataset = timeseries[-window_size:]\n        predict(test_dataset, test_loader, model, device, output_path, stop_name=stop)\n        \n        print(\"from 1021 to 1024...\")\n        inf_data = df_inf2[[c for c in df.columns if c in ['month','day','weekday','hr','min','act','tot','title']]].values.tolist()\n        test_loader = DataLoader(inf_data, shuffle=False, batch_size=1)\n        test_dataset = timeseries[937:1081]\n        predict(test_dataset, test_loader, model, device, output_path, stop_name=stop)\n        \n        print(\"Finish predicting stop \" + stop + \"\\n\")\n        i+=1\n        print(f\"Prgress {i}/112\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T06:28:31.047612Z","iopub.execute_input":"2023-12-07T06:28:31.048258Z","iopub.status.idle":"2023-12-07T06:30:23.264311Z","shell.execute_reply.started":"2023-12-07T06:28:31.048225Z","shell.execute_reply":"2023-12-07T06:30:23.262613Z"},"trusted":true},"execution_count":null,"outputs":[]}]}