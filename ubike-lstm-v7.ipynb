{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7106821,"sourceType":"datasetVersion","datasetId":4097269},{"sourceId":7107818,"sourceType":"datasetVersion","datasetId":4097960},{"sourceId":7111750,"sourceType":"datasetVersion","datasetId":4100724},{"sourceId":7112592,"sourceType":"datasetVersion","datasetId":4101363},{"sourceId":7122347,"sourceType":"datasetVersion","datasetId":4108260},{"sourceId":7122608,"sourceType":"datasetVersion","datasetId":4108448},{"sourceId":7122630,"sourceType":"datasetVersion","datasetId":4108458},{"sourceId":7122632,"sourceType":"datasetVersion","datasetId":4108460}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T20:38:32.664464Z","iopub.execute_input":"2023-12-04T20:38:32.665057Z","iopub.status.idle":"2023-12-04T20:38:33.084791Z","shell.execute_reply.started":"2023-12-04T20:38:32.665002Z","shell.execute_reply":"2023-12-04T20:38:33.083649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:33.086513Z","iopub.execute_input":"2023-12-04T20:38:33.086932Z","iopub.status.idle":"2023-12-04T20:38:36.475476Z","shell.execute_reply.started":"2023-12-04T20:38:33.086904Z","shell.execute_reply":"2023-12-04T20:38:36.474456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(dataset, lookback):\n    \"\"\"Transform a time series into a prediction dataset\n    \n    Args:\n        dataset: A numpy array of time series, first dimension is the time steps\n        lookback: Size of window for prediction\n    \"\"\"\n    X, y = [], []\n    for i in range(len(dataset)-lookback):\n        feature = dataset[i:i+lookback]\n        target = dataset[i+1:i+lookback+1]\n        X.append(feature)\n        y.append(target)\n    return torch.tensor(X), torch.tensor(y)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.477098Z","iopub.execute_input":"2023-12-04T20:38:36.477589Z","iopub.status.idle":"2023-12-04T20:38:36.483894Z","shell.execute_reply.started":"2023-12-04T20:38:36.477562Z","shell.execute_reply":"2023-12-04T20:38:36.482965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_valid_split(timeseries):\n    # train-valid split with 10/2~11/16 [0:2734], 11/17~11/30[2734:]\n    # 10/2 ~ 10/11 [:711]\n    train_size = 2734\n    valid_size = len(timeseries) - train_size\n    train, valid = timeseries[:train_size], timeseries[train_size:]\n    print(len(train), len(valid))\n    # window size approximately = 3 days\n    lookback = 72 * 2\n    X_train, y_train = create_dataset(train, lookback=lookback)\n    X_valid, y_valid = create_dataset(valid, lookback=lookback)\n    # X_test, y_test = create_dataset(test, lookback=lookback)\n    print(X_train.shape, y_train.shape)\n    print(X_valid.shape, y_valid.shape)\n    return X_train, y_train, X_valid, y_valid","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.486287Z","iopub.execute_input":"2023-12-04T20:38:36.486574Z","iopub.status.idle":"2023-12-04T20:38:36.496948Z","shell.execute_reply.started":"2023-12-04T20:38:36.486543Z","shell.execute_reply":"2023-12-04T20:38:36.495969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a basic RNN model","metadata":{}},{"cell_type":"code","source":"class UBikeRNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True)    \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        # h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n#         print(x.size(0)) # x.size(0) = batch size\n        hidden = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n        x = x.view(len(x),1,-1)\n#         print(x.shape)\n        # Passing in the input and hidden state into the model and obtaining outputs\n        out, hidden = self.rnn(x, hidden)\n        \n        # Reshaping the outputs such that it can be fit into the fully connected layer\n        out = out.contiguous().view(-1, self.hidden_dim)\n        out = self.fc(out)\n        out = self.sig(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.501410Z","iopub.execute_input":"2023-12-04T20:38:36.502330Z","iopub.status.idle":"2023-12-04T20:38:36.512763Z","shell.execute_reply.started":"2023-12-04T20:38:36.502293Z","shell.execute_reply":"2023-12-04T20:38:36.511810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBikeLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, drop_prob, output_dim):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = layer_dim\n        self.output_size = output_dim\n        \n        # initialize LSTM   \n        self.block = nn.Sequential(\n            nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Sigmoid(),\n        )\n        \n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=drop_prob, batch_first=True)\n        # the output of LSTM is classified by linear and sigmoid functions\n        self.linear = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        h_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # hidden state\n        c_0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device) # internal state\n        x = x.view(len(x),1,-1)\n        lstm_out, (hn, cn)  = self.lstm(x, (h_0.detach(), c_0.detach()))\n        hn = hn.view(-1, self.hidden_dim) # reshaping the data for Dense layer next\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.linear(lstm_out)\n        out = self.sigmoid(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.514007Z","iopub.execute_input":"2023-12-04T20:38:36.514320Z","iopub.status.idle":"2023-12-04T20:38:36.525241Z","shell.execute_reply.started":"2023-12-04T20:38:36.514295Z","shell.execute_reply":"2023-12-04T20:38:36.524208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"code","source":"def getLoss(pred, label):\n#     loss = torch.mean(3 * torch.abs(pred-label) * (torch.abs(pred-1/3) + torch.abs(pred-2/3)))\n    loss = torch.mean(torch.square(pred-label))\n#     print(f\"loss = {loss}\")\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.526725Z","iopub.execute_input":"2023-12-04T20:38:36.527014Z","iopub.status.idle":"2023-12-04T20:38:36.536939Z","shell.execute_reply.started":"2023-12-04T20:38:36.526990Z","shell.execute_reply":"2023-12-04T20:38:36.535984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 32,\n    \"data_filepath\": '/kaggle/input/stop001-sorted/500101001_sorted.csv',\n    \"inference_filepath\": '/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv',\n    \"epochs\": 500,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 5e-3,\n    \"save_dir\": \"/kaggle/working/\",\n    \"model_name\": \"stop001-RNN-v1.ckpt\",\n    \"early_stop\": 150,\n}\n# model parameters\nwindow_size = 72 * 2\ninput_dim = window_size\nhidden_dim = 512   # the hidden dim\nlayer_dim = 3   # the number of hidden layers\noutput_dim = window_size\ndrop_prob = 0.2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.538110Z","iopub.execute_input":"2023-12-04T20:38:36.538415Z","iopub.status.idle":"2023-12-04T20:38:36.569978Z","shell.execute_reply.started":"2023-12-04T20:38:36.538391Z","shell.execute_reply":"2023-12-04T20:38:36.568997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training!!!","metadata":{}},{"cell_type":"code","source":"def train(model, config, train_loader, valid_loader, device):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n\n    n_epochs, best_loss, step, early_stop_count = config['epochs'], 10000, 0, 0\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n\n        # tqdm is a package to visualize your training progress.\n#         train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_loader:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x) \n            loss = getLoss(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n#             train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n#             train_pbar.set_postfix({'loss': loss.detach().item()})\n#             print(f'Epoch [{epoch+1}/{n_epochs}]')\n#             print(f\"loss: {loss.detach().item()}\")\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = getLoss(pred, y)\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        # Note that step should be called after validate()\n        scheduler.step(mean_valid_loss)\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n            if(epoch % 10 == 0):\n                print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n            \n        if(epoch % 10 == 0):\n            print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n            if early_stop_count >= config['early_stop']:\n                print('\\nModel is not improving, so we halt the training session.')\n                print('best loss {:.3f}...'.format(best_loss))\n                return","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.571227Z","iopub.execute_input":"2023-12-04T20:38:36.571851Z","iopub.status.idle":"2023-12-04T20:38:36.598640Z","shell.execute_reply.started":"2023-12-04T20:38:36.571824Z","shell.execute_reply":"2023-12-04T20:38:36.597709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference part","metadata":{}},{"cell_type":"code","source":"def predict(test, test_loader, model, device, output_path, stop_name):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    tots = []\n    titles = []\n\n    window_size = 72 * 2\n#     test = timeseries[-window_size:].tolist()\n\n    i=0\n    month = 12\n    for month,day,weekday,hr,mins,lat,lng,act, tot, title in test_loader:\n        i+=1\n        month = month.item()\n        tot = tot.tolist()\n        title = list(title)\n        x = torch.FloatTensor(test[-window_size:])\n        x = x.view(-1,len(x))\n        x = x.to(device)      \n        tots = tots + tot\n        titles = titles + title\n        with torch.no_grad():  \n            pred = model(x)  \n            test.append(pred.tolist()[-1][-1])\n            preds.append(pred.detach().cpu())\n\n    preds = test[window_size:]\n#     print(f\"preds: {test}\")\n    print(len(preds), len(tots), len(titles))\n    plt.plot(test)\n#     plt.show()\n    plt.savefig(stop + \".png\")\n    plt.close()\n    assert len(preds) == len(tots)\n    assert len(tots) == len(titles)\n    prediction = []\n    if month == 12:\n        prediction = [['id','sbi']]\n    for (pred, tot, title) in zip(preds, tots, titles):\n        title = title.split('_')\n        real_title = title[0] + '_' + stop + '_' + title[2]\n#         print(pred, tot, pred*tot)\n        prediction.append([real_title, pred*tot])\n    \n    with open(output_path, 'a', newline='') as file:\n    # Step 4: Using csv.writer to write the list to the CSV file\n        writer = csv.writer(file)\n        writer.writerows(prediction) # Use writerows for nested list\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.604877Z","iopub.execute_input":"2023-12-04T20:38:36.605206Z","iopub.status.idle":"2023-12-04T20:38:36.618060Z","shell.execute_reply.started":"2023-12-04T20:38:36.605181Z","shell.execute_reply":"2023-12-04T20:38:36.616814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize input dataset\n# Iterate through 112 stops\ni=0\nfor dirname, _, filenames in os.walk('/kaggle/input/dataset-filled/dataset_filled'):\n    for filename in filenames:\n        stop = filename.split('.')[0]\n#         if stop != '500101035': continue\n        config[\"model_name\"] = \"stop-\" + stop + \"-RNN-v1.ckpt\"\n        output_path = 'lstm_pred_' + stop + '.csv'\n        print(\"Start training stop \" + stop + \"\\n\")\n        df = pd.read_csv('/kaggle/input/dataset-filled/dataset_filled/' + filename)\n        df_inf = pd.read_csv(\"/kaggle/input/stop001-1204-1210-inf/stop001_inf_1204_to_1210.csv\")\n        df_inf2 = pd.read_csv(\"/kaggle/input/stop001-1021-1024-inf/stop001_inf_1021_to_1024.csv\")\n        # print(len(df['ratio']))\n        # print(df['ratio'][:50])\n        timeseries = df['ratio'].values.astype('float32') \n        print(timeseries)\n        X_train, y_train, X_valid, y_valid = train_valid_split(timeseries)\n        train_loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=config['batch_size'])\n        valid_loader = DataLoader(TensorDataset(X_valid, y_valid), shuffle=True, batch_size=config['batch_size'])\n#         print(train_loader, valid_loader)\n#         model = UBikeRNN(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        model = UBikeLSTM(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, drop_prob=drop_prob, output_dim=output_dim).to(device)\n        train(model, config, train_loader, valid_loader, device)\n        print(\"Start predicting stop \" + stop + \"\\n\")\n        print(\"from 1204 to 1210...\")\n        test_data = df_inf.values.tolist()\n        print(len(test_data))\n        test_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n        test = timeseries[-window_size:].tolist()\n        predict(test, test_loader, model, device, output_path, stop_name=stop)\n        print(\"from 1021 to 1024...\")\n        test_data = df_inf2.values.tolist()\n        print(len(test_data))\n        test_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n        test = timeseries[937:1081].tolist()\n        predict(test, test_loader, model, device, output_path, stop_name=stop)\n        print(\"Finish predicting stop \" + stop + \"\\n\")\n        i+=1\n        print(f\"Prgress {i}/112\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T20:38:36.619374Z","iopub.execute_input":"2023-12-04T20:38:36.619922Z"},"trusted":true},"execution_count":null,"outputs":[]}]}