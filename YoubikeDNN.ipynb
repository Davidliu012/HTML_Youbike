<<<<<<< Updated upstream
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7093039,"sourceType":"datasetVersion","datasetId":4087683},{"sourceId":7093178,"sourceType":"datasetVersion","datasetId":4087799},{"sourceId":7099924,"sourceType":"datasetVersion","datasetId":4092622},{"sourceId":7103711,"sourceType":"datasetVersion","datasetId":4095144},{"sourceId":7104130,"sourceType":"datasetVersion","datasetId":4095417},{"sourceId":7106305,"sourceType":"datasetVersion","datasetId":4096906}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Packages","metadata":{"_uuid":"3daf6498-869d-4c38-85c9-059461e0e4f5","_cell_guid":"1da9459b-97aa-49e8-a49f-dcf71454e6be","trusted":true}},{"cell_type":"code","source":"# !pip install scikit-learn\n# !pip install numpy\n# !pip install pandas\n# !pip install torch\n# !pip install tqdm","metadata":{"_uuid":"ef6916a0-4fb6-4fa5-8ab1-e7002f8f90e8","_cell_guid":"19e73ea7-9734-48a3-8e58-998b463d823a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:46.266545Z","iopub.execute_input":"2023-12-02T14:41:46.266799Z","iopub.status.idle":"2023-12-02T14:41:46.271603Z","shell.execute_reply.started":"2023-12-02T14:41:46.266776Z","shell.execute_reply":"2023-12-02T14:41:46.270708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Packages","metadata":{"_uuid":"4e86ec41-5682-4a99-958f-78c9432de6fd","_cell_guid":"d329f981-1410-4509-93bd-06d0c0e5587f","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom tqdm import tqdm\nimport math\nimport random\nimport csv","metadata":{"_uuid":"a616ae38-2872-439e-90b4-b2933d651dcc","_cell_guid":"6f79ac2c-b8aa-4c4e-89a2-eadb1f6773b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:46.277482Z","iopub.execute_input":"2023-12-02T14:41:46.277752Z","iopub.status.idle":"2023-12-02T14:41:52.558772Z","shell.execute_reply.started":"2023-12-02T14:41:46.277728Z","shell.execute_reply":"2023-12-02T14:41:52.557977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful Functions","metadata":{"_uuid":"e860ed3f-8bce-44e1-9a78-429f6dbe4120","_cell_guid":"160c79a0-0e08-4b10-87e7-f43bc011e075","trusted":true}},{"cell_type":"markdown","source":"### Set seeds","metadata":{"_uuid":"80ee07e4-d6bb-4a9d-854a-41467981e0fa","_cell_guid":"30712a16-3f89-4c31-9e38-a1128ea15066","trusted":true}},{"cell_type":"code","source":"def same_seeds(seed):\n    random.seed(seed) \n    np.random.seed(seed)  \n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","metadata":{"_uuid":"eaa743f2-5526-4764-af65-f369102c673b","_cell_guid":"983e1f65-4f48-400f-ac54-802c107405bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.560566Z","iopub.execute_input":"2023-12-02T14:41:52.561100Z","iopub.status.idle":"2023-12-02T14:41:52.567341Z","shell.execute_reply.started":"2023-12-02T14:41:52.561062Z","shell.execute_reply":"2023-12-02T14:41:52.566193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Dataset from CSV Files","metadata":{"_uuid":"396cd262-6b3a-4a4b-ba53-8fadab4d4e7d","_cell_guid":"e4127685-9339-4652-8ac5-d3fd29d18a36","trusted":true}},{"cell_type":"code","source":"def preprocessData(previousData, data, dataType):\n    preprocessedData = []\n    currentDate = []\n    dateData = []\n    dateLabel = []\n    \n    for singleData in data:\n        singleData[5] -= 24.5\n        singleData[6] -= 121\n        singleData[4] /= 60\n        if len(currentDate) == 0:\n            currentDate = [singleData[0], singleData[1], singleData[2], singleData[3], singleData[4]]\n            dateData.append([singleData[5], singleData[6], singleData[7]])\n            dateLabel.append(singleData[8])\n            if dataType == 'test':\n                dateLabel.append(singleData[9])\n        elif currentDate == [singleData[0], singleData[1], singleData[2], singleData[3], singleData[4]]:\n            dateData.append([singleData[5], singleData[6], singleData[7]])\n            dateLabel.append(singleData[8])\n            if dataType == 'test':\n                dateLabel.append(singleData[9])\n        else:\n            dateData = list(np.concatenate(dateData).flat)\n            preprocessedData.append(dateData+currentDate+dateLabel)\n            currentDate = [singleData[0], singleData[1], singleData[2], singleData[3], singleData[4]]\n            dateData = [[singleData[5], singleData[6], singleData[7]]]\n            dateLabel = [singleData[8]]\n            if dataType == 'test':\n                dateLabel.append(singleData[9])\n                \n    dateData = list(np.concatenate(dateData).flat)\n    preprocessedData.append(dateData+currentDate+dateLabel)\n    \n    for singlePreprocessedData in preprocessedData:\n        previousData.append(singlePreprocessedData)\n    return previousData\n\ndef readDataset(data_filepath, inference_filepath):\n    assert os.path.exists(data_filepath)\n    filenames = os.listdir(data_filepath)\n    if '.DS_Store' in filenames:\n        filenames.remove('.DS_Store')\n    filenames = sorted(filenames)\n    train, valid, test = [], [], []\n    for idx, filename in enumerate(filenames):\n        data = (pd.read_csv(data_filepath + filename).values).tolist()\n        if idx >= len(filenames) - 14:\n            # validation\n            valid = preprocessData(valid, data, 'valid')\n        else:\n            #training\n            train = preprocessData(train, data, 'train')\n    \n    assert os.path.exists(inference_filepath)\n    filenames = os.listdir(inference_filepath)\n    if '.DS_Store' in filenames:\n        filenames.remove('.DS_Store')\n    filenames = sorted(filenames, reverse=True)\n    test = []\n    for idx, filename in enumerate(filenames):\n        print(filename)\n        data = (pd.read_csv(inference_filepath + filename).values).tolist()\n        test = preprocessData(test, data, 'test')\n    return train, valid, test","metadata":{"_uuid":"fd04738f-6f2b-44ab-a590-be3da6a10d4b","_cell_guid":"35f220f2-1249-408b-825a-96e55a1c67e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.568531Z","iopub.execute_input":"2023-12-02T14:41:52.568814Z","iopub.status.idle":"2023-12-02T14:41:52.585560Z","shell.execute_reply.started":"2023-12-02T14:41:52.568789Z","shell.execute_reply":"2023-12-02T14:41:52.584683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Youbike Dataset Class","metadata":{"_uuid":"47be862a-ead4-4c40-be1c-57099c557c16","_cell_guid":"99467969-70fa-4934-8864-746e19831d3a","trusted":true}},{"cell_type":"code","source":"class YoubikeDataset(Dataset):\n    def __init__(self, data, dataType):\n        super(YoubikeDataset, self).__init__()\n        # [month, date, weekday, hr, min, lat, lng, act, ratio, sbi, tot, title, act_title]\n        # [month, day ,weekday ,hr ,min ,lat ,lng ,act ,tot ,title ]\n        self.data = data\n        self.datasize = len(self.data)\n        self.type = dataType\n\n    def __getitem__(self, idx):\n        if self.type == \"train\" or self.type == \"val\":\n            features = self.data[idx][:-112]\n            labels = self.data[idx][-112:]\n            return torch.FloatTensor(features), torch.FloatTensor(labels)\n        elif self.type == \"test\":\n            features = self.data[idx][:-224]\n            outputInfo = self.data[idx][-224:]\n            return torch.FloatTensor(features), outputInfo\n        else:\n            raise NotImplementedError\n            \n    def __len__(self):\n        return self.datasize","metadata":{"_uuid":"5cb83efb-01ca-4808-9c64-d339112a88fd","_cell_guid":"98766ab9-dc06-40cc-bd41-90a650baa3bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.587735Z","iopub.execute_input":"2023-12-02T14:41:52.588014Z","iopub.status.idle":"2023-12-02T14:41:52.603893Z","shell.execute_reply.started":"2023-12-02T14:41:52.587989Z","shell.execute_reply":"2023-12-02T14:41:52.603189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Model(s)","metadata":{"_uuid":"4eac3393-99bb-4326-9c09-16cd1f2dac43","_cell_guid":"420e9d0b-95a7-4047-9656-b874e3217306","trusted":true}},{"cell_type":"markdown","source":"### DNN model","metadata":{"_uuid":"9794c4b8-a152-4f8b-a702-3af6ab50ce97","_cell_guid":"6104fbaa-4599-453a-8a89-9ee3b4be7b12","trusted":true}},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(My_Model, self).__init__()\n        # TODO: modify model's structure, be aware of dimensions. \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(input_dim, 2*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(2*input_dim, 4*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(4*input_dim, 8*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(8*input_dim, 8*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(8*input_dim, 4*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(), \n            nn.Linear(4*input_dim, 2*input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(2*input_dim, input_dim),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(input_dim, output_dim*2),\n            nn.Dropout(p=0.1),\n            nn.ReLU(),\n            nn.Linear(output_dim*2, output_dim),\n#             nn.Dropout(p=0.1),\n            nn.Sigmoid(),\n#             nn.Linear(input_dim, 4),\n#             nn.Sigmoid(),\n#             nn.Linear(4, 2),\n#             nn.Sigmoid(),\n#             nn.Linear(2, 1),\n#             nn.Sigmoid(),\n#             nn.Linear(input_dim, 1),\n#             nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x","metadata":{"_uuid":"1f21a170-1062-4c0b-98ba-62f9d15f538f","_cell_guid":"a3cfbb1c-1ee7-4fff-972c-cf650e731c0b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.604935Z","iopub.execute_input":"2023-12-02T14:41:52.605218Z","iopub.status.idle":"2023-12-02T14:41:52.625607Z","shell.execute_reply.started":"2023-12-02T14:41:52.605181Z","shell.execute_reply":"2023-12-02T14:41:52.624747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Loss Function","metadata":{"_uuid":"67437db6-a6a8-436e-b180-f5a7207bc051","_cell_guid":"c93605b9-e61a-4406-ac07-9af124a892ce","trusted":true}},{"cell_type":"code","source":"def getLoss(pred, label):\n    loss = torch.mean(torch.square(pred-label))\n#     print(f\"loss = {loss}\")\n    return loss","metadata":{"_uuid":"347621a1-bfc7-4cef-a557-2fb42fd2b279","_cell_guid":"8a261f5c-9b4f-4c11-b5c6-544de13f797c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.626703Z","iopub.execute_input":"2023-12-02T14:41:52.626951Z","iopub.status.idle":"2023-12-02T14:41:52.643553Z","shell.execute_reply.started":"2023-12-02T14:41:52.626930Z","shell.execute_reply":"2023-12-02T14:41:52.642723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training function","metadata":{"_uuid":"c2c5b894-c457-4860-b5e1-05f0b7fa18fc","_cell_guid":"475a5f4e-e4ab-46d7-94f3-75e1159a9379","trusted":true}},{"cell_type":"code","source":"def train(model, config, train_loader, valid_loader, device):\n#     criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n    # criterion = nn.CrossEntropyLoss()\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=1,T_mult=2)\n    if not os.path.isdir(config[\"save_dir\"]):\n        os.mkdir(config[\"save_dir\"]) # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count = config['epochs'], math.inf, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n        print(scheduler.get_last_lr())\n\n        # tqdm is a package to visualize your training progress.\n        train_pbar = tqdm(train_loader, position=0, leave=True)\n        final_y, final_pred = None, None\n        for x, y in train_pbar:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x) \n            \n            loss = getLoss(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # Display current epoch number and loss on tqdm progress bar.\n            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n            train_pbar.set_postfix({'loss': loss.detach().item()})\n            final_y = y\n            final_pred = pred\n\n        scheduler.step()\n        mean_train_loss = sum(loss_record)/len(loss_record)\n        print(final_y)\n        print(final_pred)\n\n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)\n                loss = getLoss(pred, y)\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n            print('Saving model with loss {:.4f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n\n        if early_stop_count >= config['early_stop']:\n            print('\\nModel is not improving, so we halt the training session.')\n            print('best loss {:.4f}...'.format(best_loss))\n            return","metadata":{"_uuid":"ca28bea5-8dc0-483d-8571-c46ecac15d8d","_cell_guid":"06d3556a-ad61-4ccf-a9c7-62ab152e90df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.644729Z","iopub.execute_input":"2023-12-02T14:41:52.644985Z","iopub.status.idle":"2023-12-02T14:41:52.660110Z","shell.execute_reply.started":"2023-12-02T14:41:52.644962Z","shell.execute_reply":"2023-12-02T14:41:52.659153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict function","metadata":{"_uuid":"df41ce06-1683-434e-b250-6969e3bf7230","_cell_guid":"1560181e-e304-48bf-bdb8-03366dc0b154","trusted":true}},{"cell_type":"code","source":"def predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    tots = []\n    titles = []\n    for x, tmp in tqdm(test_loader):\n        x = x.to(device)   \n        infos = []\n        for i in range(len(tmp[0])):\n            lst = [tmp[x][i] for x in range(len(tmp)) ]\n            infos = infos + lst\n        for idx, info in enumerate(infos):\n            if idx % 2 == 0:\n                tots.append(info)\n            else:\n                titles.append(info)\n        with torch.no_grad():                   \n            pred = model(x)                     \n            preds.append(pred.detach().cpu())\n            \n    preds = list(np.concatenate(preds).flat)\n    print(len(preds), len(tots), len(titles))\n    assert len(preds) == len(tots)\n    assert len(tots) == len(titles)\n    prediction = [['id','sbi']]\n    for (pred, tot, title) in zip(preds, tots, titles):\n        prediction.append([title, pred*tot.item()])\n    \n    with open('prediction.csv', 'w', newline='') as file:\n    # Step 4: Using csv.writer to write the list to the CSV file\n        writer = csv.writer(file)\n        writer.writerows(prediction) # Use writerows for nested list\n    \n    return","metadata":{"_uuid":"fcd12733-6b47-43a7-8b69-90d75b072dd6","_cell_guid":"60941160-9ba4-47e7-aad8-f227f2359b73","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T15:14:13.111715Z","iopub.execute_input":"2023-12-02T15:14:13.112113Z","iopub.status.idle":"2023-12-02T15:14:13.122590Z","shell.execute_reply.started":"2023-12-02T15:14:13.112083Z","shell.execute_reply":"2023-12-02T15:14:13.121655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"_uuid":"77ee2261-8391-4a2a-a593-6a69d5114ee0","_cell_guid":"0c055bfc-067a-4b4c-9b11-a314cdb3a32d","trusted":true}},{"cell_type":"code","source":"config = {\n    \"batch_size\": 2,\n#     \"data_filepath\": 'dataset_csv/',\n    \"data_filepath\": '/kaggle/input/dataset-1201-new/dataset_w_csv/',\n#     \"inference_filepath\": '/kaggle/input/inference-1204/inference_csv/',\n    \"inference_filepath\": '/kaggle/input/inference-new-1204/inference_w_csv/',\n    \"epochs\": 100,\n    \"learning_rate\": 2.5e-3,\n#     \"weight_decay\": 5e-4,\n    \"weight_decay\": 0,\n    \"save_dir\": \"./models/\",\n    \"model_name\": \"1201-DNN.ckpt\",\n    \"checkpoint\": \"/kaggle/input/300-ckpt/1201-DNN.ckpt\",\n    \"useCheckpoint\": True,\n    \"early_stop\":100,\n    \"seeds\": 10901039\n}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"f7b481f8-bdb2-4eff-98b4-30aa684bddee","_cell_guid":"5b8c80e4-d544-41bc-a37b-898f74bdb50f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T15:26:33.911350Z","iopub.execute_input":"2023-12-02T15:26:33.911733Z","iopub.status.idle":"2023-12-02T15:26:33.917697Z","shell.execute_reply.started":"2023-12-02T15:26:33.911708Z","shell.execute_reply":"2023-12-02T15:26:33.916786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Datasets","metadata":{"_uuid":"cad4ef4f-ba16-4919-9a9b-7e29ac6ab21e","_cell_guid":"719398f7-831c-432c-8a32-54b13ca2b940","trusted":true}},{"cell_type":"code","source":"same_seeds(config[\"seeds\"])\ntrain_data, valid_data, test_data = readDataset(config['data_filepath'], config['inference_filepath'])\nprint(f'train_data_size: {len(train_data)}')\nprint(f'valid_data_size: {len(valid_data)}')\nprint(f'test_data_size : {len(test_data)}')\ntrain_dataset, valid_dataset, test_dataset = YoubikeDataset(train_data, \"train\"), YoubikeDataset(valid_data, \"val\"), YoubikeDataset(test_data, \"test\")\ntrain_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\ntest_loader  = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)","metadata":{"_uuid":"1630ec01-2dcb-4730-8b86-25c78a1032fb","_cell_guid":"724cb34d-0f6a-4715-b1ae-a13acd9e1b32","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T14:41:52.766472Z","iopub.execute_input":"2023-12-02T14:41:52.766762Z","iopub.status.idle":"2023-12-02T14:41:57.223171Z","shell.execute_reply.started":"2023-12-02T14:41:52.766737Z","shell.execute_reply":"2023-12-02T14:41:57.222298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Checkpoint","metadata":{}},{"cell_type":"code","source":"model = My_Model(input_dim=len(train_data[0])-112, output_dim=112).to(device) # put your model and data on the same computation device.\nif config['useCheckpoint']:\n    model.load_state_dict(torch.load(config['checkpoint']))\n    \nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T15:21:12.109094Z","iopub.execute_input":"2023-12-02T15:21:12.109510Z","iopub.status.idle":"2023-12-02T15:21:12.939720Z","shell.execute_reply.started":"2023-12-02T15:21:12.109463Z","shell.execute_reply":"2023-12-02T15:21:12.938741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training","metadata":{"_uuid":"a67e9f5a-9cea-4fa1-9265-edd6620f1ba4","_cell_guid":"d6504785-d164-417e-9945-c223660f0b76","trusted":true}},{"cell_type":"code","source":"train(model, config, train_loader, valid_loader, device)","metadata":{"_uuid":"d80900d2-5ac2-4b39-ba7e-f4ae8d404e73","_cell_guid":"87385bfc-3634-4f6c-8580-0fa459f07111","execution":{"iopub.status.busy":"2023-12-02T15:26:38.988643Z","iopub.execute_input":"2023-12-02T15:26:38.989347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"6005ee34-f764-4696-9167-740dedd54fad","_cell_guid":"87c72eb7-4c92-4558-967b-10c3d39418b4","trusted":true}},{"cell_type":"code","source":"predict(test_loader, model, device)","metadata":{"_uuid":"32c5adff-6cef-4bf7-93f1-6092c5307665","_cell_guid":"cf8b912f-284c-4b20-bfa7-5e216b4e93f6","execution":{"iopub.status.busy":"2023-12-02T15:21:18.633450Z","iopub.execute_input":"2023-12-02T15:21:18.634584Z","iopub.status.idle":"2023-12-02T15:21:21.358415Z","shell.execute_reply.started":"2023-12-02T15:21:18.634538Z","shell.execute_reply":"2023-12-02T15:21:21.357634Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
=======
{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Install Packages "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T18:51:49.306599Z","iopub.status.busy":"2023-11-30T18:51:49.305786Z","iopub.status.idle":"2023-11-30T18:51:49.310864Z","shell.execute_reply":"2023-11-30T18:51:49.309912Z","shell.execute_reply.started":"2023-11-30T18:51:49.306556Z"},"trusted":true},"outputs":[],"source":["# !pip install scikit-learn\n","# !pip install numpy\n","# !pip install pandas\n","# !pip install torch\n","# !pip install tqdm"]},{"cell_type":"markdown","metadata":{},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3886c10b-f7c4-47cf-8857-2f30cdcedb45","_uuid":"c82b120e-39f1-4533-9688-3a0b859a062f","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:51:49.313403Z","iopub.status.busy":"2023-11-30T18:51:49.312810Z","iopub.status.idle":"2023-11-30T18:51:49.323658Z","shell.execute_reply":"2023-11-30T18:51:49.322814Z","shell.execute_reply.started":"2023-11-30T18:51:49.313367Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","from tqdm import tqdm\n","import math\n","import random\n","import csv"]},{"cell_type":"markdown","metadata":{},"source":["## Useful Functions "]},{"cell_type":"markdown","metadata":{},"source":["### Set seeds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T18:51:49.325898Z","iopub.status.busy":"2023-11-30T18:51:49.325554Z","iopub.status.idle":"2023-11-30T18:51:49.333977Z","shell.execute_reply":"2023-11-30T18:51:49.333145Z","shell.execute_reply.started":"2023-11-30T18:51:49.325867Z"},"trusted":true},"outputs":[],"source":["def same_seeds(seed):\n","    random.seed(seed) \n","    np.random.seed(seed)  \n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed) \n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{},"source":["### Read Dataset from CSV Files"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"590ecbce-8877-4ca8-ae7f-d7d83992502b","_uuid":"34f64581-b863-4aed-90cf-714ec8edcd5e","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:51:49.335568Z","iopub.status.busy":"2023-11-30T18:51:49.335301Z","iopub.status.idle":"2023-11-30T18:51:49.345276Z","shell.execute_reply":"2023-11-30T18:51:49.344426Z","shell.execute_reply.started":"2023-11-30T18:51:49.335544Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def readDataset(data_filepath, inference_filepath):\n","    assert os.path.exists(data_filepath)\n","    filenames = os.listdir(data_filepath)\n","    if '.DS_Store' in filenames:\n","        filenames.remove('.DS_Store')\n","    filenames = sorted(filenames)\n","    train, valid = [], []\n","    \n","    for idx, filename in enumerate(filenames):\n","        data = (pd.read_csv(data_filepath + filename).values).tolist()\n","        if idx >= len(filenames) - 14:\n","            # validation\n","            for single_data in data:\n","                valid.append(single_data)\n","        else:\n","            #training\n","            for single_data in data:\n","                train.append(single_data)\n","    \n","    assert os.path.exists(inference_filepath)\n","    filenames = os.listdir(inference_filepath)\n","    if '.DS_Store' in filenames:\n","        filenames.remove('.DS_Store')\n","    filenames = sorted(filenames, reverse=True)\n","    test = []\n","    for idx, filename in enumerate(filenames):\n","        print(filename)\n","        data = (pd.read_csv(inference_filepath + filename).values).tolist()\n","        for single_data in data:\n","            test.append(single_data)\n","    return train, valid, test"]},{"cell_type":"markdown","metadata":{},"source":["## My Youbike Dataset Class "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1f743cb-4af8-40f9-8491-c97b92210c97","_uuid":"24f46a6f-ebb4-4cab-b211-834442d0d02e","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:51:49.346591Z","iopub.status.busy":"2023-11-30T18:51:49.346340Z","iopub.status.idle":"2023-11-30T18:51:49.358438Z","shell.execute_reply":"2023-11-30T18:51:49.357603Z","shell.execute_reply.started":"2023-11-30T18:51:49.346569Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class YoubikeDataset(Dataset):\n","    def __init__(self, data, dataType):\n","        super(YoubikeDataset, self).__init__()\n","        # [month, date, weekday, hr, min, lat, lng, act, ratio, sbi, tot, title, act_title]\n","        # [month, day ,weekday ,hr ,min ,lat ,lng ,act ,tot ,title ]\n","        self.data = data\n","        self.datasize = len(self.data)\n","        self.type = dataType\n","\n","    def __getitem__(self, idx):\n","        features = self.data[idx][:8]\n","        if self.type == \"train\" or self.type == \"val\":\n","            label = [self.data[idx][8]]\n","            return torch.FloatTensor(features), torch.FloatTensor(label)\n","        elif self.type == \"test\":\n","            total = self.data[idx][8]\n","            title = self.data[idx][-1]\n","            return torch.FloatTensor(features), total, title\n","        else:\n","            raise NotImplementedError\n","            \n","    def __len__(self):\n","        return self.datasize"]},{"cell_type":"markdown","metadata":{},"source":["## My Model(s)"]},{"cell_type":"markdown","metadata":{},"source":["### DNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9b009bb-f67b-4740-9bf0-14c4c57c597c","_uuid":"8064e40a-7ebd-4adc-a871-8786bb42c7b1","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:51:49.360303Z","iopub.status.busy":"2023-11-30T18:51:49.360058Z","iopub.status.idle":"2023-11-30T18:51:49.369294Z","shell.execute_reply":"2023-11-30T18:51:49.368485Z","shell.execute_reply.started":"2023-11-30T18:51:49.360278Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class My_Model(nn.Module):\n","    def __init__(self, input_dim):\n","        super(My_Model, self).__init__()\n","        # TODO: modify model's structure, be aware of dimensions. \n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, 2*input_dim),\n","#             nn.Dropout(p=0.1),\n","            nn.Sigmoid(),\n","            nn.Linear(2*input_dim, 4*input_dim),\n","#             nn.Dropout(p=0.1),\n","            nn.Sigmoid(),\n","            nn.Linear(4*input_dim, 2*input_dim),\n","#             nn.Dropout(p=0.1),\n","            nn.Sigmoid(),\n","            nn.Linear(2*input_dim, input_dim),\n","#             nn.Dropout(p=0.1),\n","            nn.Sigmoid(),\n","            nn.Linear(input_dim, 4),\n","            nn.Sigmoid(),\n","            nn.Linear(4, 2),\n","            nn.Sigmoid(),\n","            nn.Linear(2, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## My Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T18:51:49.370483Z","iopub.status.busy":"2023-11-30T18:51:49.370245Z","iopub.status.idle":"2023-11-30T18:51:49.384311Z","shell.execute_reply":"2023-11-30T18:51:49.383503Z","shell.execute_reply.started":"2023-11-30T18:51:49.370462Z"},"trusted":true},"outputs":[],"source":["def getLoss(pred, label):\n","    loss = torch.mean(3*torch.abs(pred-label)*(torch.abs(pred-1/3)+torch.abs(pred-2/3)))\n","#     print(f\"loss = {loss}\")\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["## Training function"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8eb27ae9-f1a0-48ec-b592-b30bbd58c010","_uuid":"86c145bf-8323-4e9b-9896-fd1a6fa34695","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:51:49.459796Z","iopub.status.busy":"2023-11-30T18:51:49.459516Z","iopub.status.idle":"2023-11-30T18:51:49.472699Z","shell.execute_reply":"2023-11-30T18:51:49.471661Z","shell.execute_reply.started":"2023-11-30T18:51:49.459773Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def train(model, config, train_loader, valid_loader, device):\n","#     criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n","    # criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n","    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=1,T_mult=2)\n","    if not os.path.isdir(config[\"save_dir\"]):\n","        os.mkdir(config[\"save_dir\"]) # Create directory of saving models.\n","\n","    n_epochs, best_loss, step, early_stop_count = config['epochs'], math.inf, 0, 0\n","\n","    for epoch in range(n_epochs):\n","        model.train() # Set your model to train mode.\n","        loss_record = []\n","        print(scheduler.get_last_lr())\n","\n","        # tqdm is a package to visualize your training progress.\n","        train_pbar = tqdm(train_loader, position=0, leave=True)\n","\n","        for x, y in train_pbar:\n","            optimizer.zero_grad()               # Set gradient to zero.\n","            x, y = x.to(device), y.to(device)   # Move your data to device. \n","            pred = model(x) \n","            loss = getLoss(pred, y)\n","            loss.backward()                     # Compute gradient(backpropagation).\n","            optimizer.step()                    # Update parameters.\n","            step += 1\n","            loss_record.append(loss.detach().item())\n","            \n","            # Display current epoch number and loss on tqdm progress bar.\n","            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n","            train_pbar.set_postfix({'loss': loss.detach().item()})\n","\n","        scheduler.step()\n","        mean_train_loss = sum(loss_record)/len(loss_record)\n","\n","        model.eval() # Set your model to evaluation mode.\n","        loss_record = []\n","        for x, y in valid_loader:\n","            x, y = x.to(device), y.to(device)\n","            with torch.no_grad():\n","                pred = model(x)\n","                loss = getLoss(pred, y)\n","\n","            loss_record.append(loss.item())\n","            \n","        mean_valid_loss = sum(loss_record)/len(loss_record)\n","        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n","        # writer.add_scalar('Loss/valid', mean_valid_loss, step)\n","\n","        if mean_valid_loss < best_loss:\n","            best_loss = mean_valid_loss\n","            torch.save(model.state_dict(), config['save_dir'] + config['model_name']) # Save your best model\n","            print('Saving model with loss {:.3f}...'.format(best_loss))\n","            early_stop_count = 0\n","        else: \n","            early_stop_count += 1\n","\n","        if early_stop_count >= config['early_stop']:\n","            print('\\nModel is not improving, so we halt the training session.')\n","            print('best loss {:.3f}...'.format(best_loss))\n","            return"]},{"cell_type":"markdown","metadata":{},"source":["## Predict function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T18:51:49.474611Z","iopub.status.busy":"2023-11-30T18:51:49.474349Z","iopub.status.idle":"2023-11-30T18:51:49.485960Z","shell.execute_reply":"2023-11-30T18:51:49.485231Z","shell.execute_reply.started":"2023-11-30T18:51:49.474588Z"},"trusted":true},"outputs":[],"source":["def predict(test_loader, model, device):\n","    model.eval() # Set your model to evaluation mode.\n","    preds = []\n","    tots = []\n","    titles = []\n","    for x, tot, title in tqdm(test_loader):\n","        tot = tot.tolist()\n","        x = x.to(device)      \n","        tots = tots + tot\n","        titles = titles + title\n","        with torch.no_grad():                   \n","            pred = model(x)                     \n","            preds.append(pred.detach().cpu())\n","            \n","    preds = torch.cat(preds, dim=0).numpy().tolist()\n","    print(len(preds), len(tots), len(titles))\n","    assert len(preds) == len(tots)\n","    assert len(tots) == len(titles)\n","    prediction = [['id','sbi']]\n","    for (pred, tot, title) in zip(preds, tots, titles):\n","        \n","        prediction.append([title, pred[0]*tot])\n","    \n","    with open('prediction.csv', 'w', newline='') as file:\n","    # Step 4: Using csv.writer to write the list to the CSV file\n","        writer = csv.writer(file)\n","        writer.writerows(prediction) # Use writerows for nested list\n","    \n","    return"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"050181da-4d1e-4cce-addc-868287bbf465","_uuid":"1642af7c-4541-401c-97cd-fd4aed575e53","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:52:19.998026Z","iopub.status.busy":"2023-11-30T18:52:19.997123Z","iopub.status.idle":"2023-11-30T18:52:20.004453Z","shell.execute_reply":"2023-11-30T18:52:20.003389Z","shell.execute_reply.started":"2023-11-30T18:52:19.997976Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["config = {\n","    \"batch_size\": 32,\n","#     \"data_filepath\": 'dataset_csv/',\n","    \"data_filepath\": '/kaggle/input/dataset-1201/dataset_csv/',\n","    \"inference_filepath\": '/kaggle/input/inference-1204/inference_csv/',\n","    \"epochs\": 50,\n","    \"learning_rate\": 1,\n","    \"weight_decay\": 5e-3,\n","    \"save_dir\": \"./models/\",\n","    \"model_name\": \"1201-DNN.ckpt\",\n","    \"early_stop\":50,\n","    \"seeds\": 10901036\n","}\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["## Load Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58d5efaf-76ee-4332-9085-e6d1b623cfc5","_uuid":"c80b8a6f-f869-40d3-9822-05a3747b9ceb","collapsed":false,"execution":{"iopub.execute_input":"2023-11-30T18:52:20.006710Z","iopub.status.busy":"2023-11-30T18:52:20.006327Z","iopub.status.idle":"2023-11-30T18:52:21.973835Z","shell.execute_reply":"2023-11-30T18:52:21.972911Z","shell.execute_reply.started":"2023-11-30T18:52:20.006677Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["same_seeds(config[\"seeds\"])\n","train_data, valid_data, test_data = readDataset(config['data_filepath'], config['inference_filepath'])\n","print(f'train_data_size: {len(train_data)}')\n","print(f'valid_data_size: {len(valid_data)}')\n","print(f'test_data_size : {len(test_data)}')\n","train_dataset, valid_dataset, test_dataset = YoubikeDataset(train_data, \"train\"), YoubikeDataset(valid_data, \"val\"), YoubikeDataset(test_data, \"test\")\n","train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n","test_loader  = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"192b0a7e-9672-43b9-ae6c-b01f7f217a2f","_uuid":"0c61441f-52c7-4ecc-a3ac-d8c75362120a","execution":{"iopub.execute_input":"2023-11-30T18:52:21.975838Z","iopub.status.busy":"2023-11-30T18:52:21.975443Z"},"trusted":true},"outputs":[],"source":["model = My_Model(input_dim=8).to(device) # put your model and data on the same computation device.\n","\n","train(model, config, train_loader, valid_loader, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict(test_loader, model, device)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4083080,"sourceId":7086646,"sourceType":"datasetVersion"},{"datasetId":4087683,"sourceId":7093039,"sourceType":"datasetVersion"},{"datasetId":4087799,"sourceId":7093178,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
>>>>>>> Stashed changes
